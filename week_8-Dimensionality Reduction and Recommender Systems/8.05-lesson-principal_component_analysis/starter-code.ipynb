{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Principal Component Analysis\n",
    "\n",
    "> Creator: Matt Brems (DC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "By the end of the lesson, students should be able to:\n",
    "1. Differentiate between feature elimination and feature extraction.\n",
    "2. Describe the PCA algorithm.\n",
    "3. Identify use cases for PCA.\n",
    "4. Implement PCA in `scikit-learn`.\n",
    "5. Calculate and interpret proportion of explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of Problem\n",
    "\n",
    "Today, we're going to be using data from the [American Community Survey](https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml) (you can think of it as similar to the U.S. Census, but the ACS is administered more frequently than every ten years).\n",
    "\n",
    "Specifically, we are going to look household data (how many households have married couples, how many households have children under 18, how many households have children under 6, etc. etc.) in 2016 and use these factors to predict the population in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is a bit of data munging. I'll briefly summarize this, but you can check it out in more detail later if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in 2017 CA population data.\n",
    "pop = pd.read_csv('./datasets/2017/ACS_17_1YR_S0101_with_ann.csv', header=1)\n",
    "\n",
    "## Drop all columns except for county and population size.\n",
    "pop = pop[['Geography', 'Total; Estimate; Total population']]\n",
    "\n",
    "## Read in 2016 CA household data.\n",
    "hh_2016 = pd.read_csv('./datasets/2016/ACS_16_1YR_S1101_with_ann.csv', header=1)\n",
    "\n",
    "## Remove columns that contain margins of error, percentages, and IDs.\n",
    "## American Community Survey and Census data usually have many, many columns.\n",
    "hh_drop = [i for i in hh_2016.columns if 'Margin of Error' in i or 'Percent' in i]\n",
    "hh_drop.append('Id')\n",
    "hh_drop.append('Id2')\n",
    "hh_2016.drop(columns=hh_drop, inplace = True)\n",
    "\n",
    "## Merge the 2017 population data with the 2016 household data on county.\n",
    "df = pd.merge(pop, hh_2016, how = 'inner', on = 'Geography')\n",
    "\n",
    "## Rename Y column.\n",
    "df.columns = ['pop_2017' if i == 'Total; Estimate; Total population' else i for i in df.columns]\n",
    "\n",
    "## Set index to be the county.\n",
    "df.set_index('Geography', inplace = True)\n",
    "\n",
    "## Drop **columns** with NaN.\n",
    "df.replace(to_replace = '(X)', value = np.nan, inplace = True)\n",
    "df.dropna(axis = 1, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a multiple linear regression model in `sklearn`.\n",
    "- Import Linear Regression.\n",
    "- Set `y` to be the `pop_2017` column.\n",
    "- Use all other columns as your `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>**Check:** What is the problem with this?</summary>\n",
    "- We have more columns than we have rows! We will be able to come up with a perfect fit for our model, but our model probably isn't actually perfect!\n",
    "</details>\n",
    "\n",
    "<details><summary>**Check:** How can we overcome this problem?</summary>\n",
    "- We can drop features from our model. (However, this loses any benefit we'd get from dropping those features!)\n",
    "- Maybe we can combine features together so that we can get the benefits of most/all of our features. (This is what PCA will do.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction refers to **reducing the dimension of our feature space**. Less technically, this is (approximately) the number of features we use in our model.\n",
    "\n",
    "#### Dimensionality reduction has a number of advantages:\n",
    "- Increases computational efficiency when fitting models.\n",
    "- Can help with addressing a multicollinearity problem.\n",
    "- Makes visualization simpler (or feasible).\n",
    "\n",
    "#### Dimensionality reduction can suffer from some drawbacks, though:\n",
    "- We've invested our time and money into collecting information... why do we want to get rid of it?\n",
    "\n",
    "Dimensionality reduction can generally be broken down into one of two categories:\n",
    "- **Feature Elimination**\n",
    "    - In feature elimination, we drop variables from our model.\n",
    "- **Feature Extraction**\n",
    "    - In feature extraction, we take our existing features and combine them together in a particular way. We can then drop some of these \"new\" variables, but the variables we keep are still a combination of the old variables!\n",
    "    - This allows us to still reduce the number of features in our model **but** we can keep all of the most important pieces of the original features!\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "X_1, \\ldots, X_p &\\Rightarrow& Z_1, \\ldots, Z_p \\\\\n",
    "\\\\\n",
    "\\text{most important: }Z_1 &=& w_{0,1} + w_{1,1}X_1 + \\cdots + w_{p,1}X_p \\\\\n",
    "\\text{slightly less important: }Z_2 &=& w_{0,2} + w_{1,2}X_1 + \\cdots + w_{p,2}X_p \\\\\n",
    "\\text{least important: }Z_p &=& w_{0,p} + w_{1,3}X_1 + \\cdots + w_{p,3}X_p\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "You can think of $Z_1$ as a **high performance predictor** (not a real term), where $Z_1$ has all of the best pieces of $X_1$ through $X_p$. As we move down the list toward $Z_p$, the variables will consist of the more \"redundant\" parts of our $X$ variables.\n",
    "- Remember the \"sweet potato\" analogy, where variables all get in line to take a bite of the variability in our data? And then once that bite is gone, no other variable can take that bite (a.k.a. explain that variability)? $Z_1$ consists of the parts of $X_1$ through $X_p$ that all get their own chunk of the sweet potato. On the other hand, $Z_p$ will consist of the redundant pieces.\n",
    "\n",
    "Dimensionality reduction can be used as an exploratory/unsupervised learning method or as a pre-processing step for supervised learning later.\n",
    "\n",
    "**Principal component analysis** is a method used for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "### Big picture, what is PCA doing?\n",
    "1. We are going to look at how all of the $X$ variables relate to one another, then summarize these relationships.\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important.\n",
    "3. We can also see exactly how important each combination is, then rank these combinations.\n",
    "\n",
    "Once we've taken our original $X$ data and transformed it into $Z$, we can then drop the columns of $Z$ that are \"least important.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Essentially...**\n",
    "\n",
    "- PCA finds *linear combinations* of current predictor variables that...\n",
    "- create new \"principal components\". The principal components explain...\n",
    "- the maximum possible amount of variance in your predictors.\n",
    "\n",
    "$$ PC_1 = w_{1,1}X_1 + w_{2,1}X_2 + \\cdots + w_{p,1}X_p $$\n",
    "\n",
    "$$ PC_2 = w_{1,2}X_1 + w_{2,2}X_2 + \\cdots + w_{p,2}X_p $$\n",
    "\n",
    "$$ PC_3 = w_{1,3}X_1 + w_{2,3}X_2 + \\cdots + w_{p,3}X_p $$\n",
    "\n",
    "This is cool because...\n",
    "\n",
    "- $PC1$ is better than $PC2$ is \"better\" than $PC3$  \n",
    "- All of these are *uncorrelated*\n",
    "\n",
    "---\n",
    "\n",
    "**Visually...**\n",
    "\n",
    "> Think of our data floating out in $p$-dimensional space. Each observation is a dot and you can imagine this massive cloud of dots that exists somewhere. PCA is a way to rotate this cloud of dots (formally, a coordinate transformation). The old axes are the original features. The new axes are the principal components from PCA.\n",
    "\n",
    "**The new axes (principal components) become the most concise, informative descriptors of our data as a whole.**\n",
    "\n",
    "Let's head to [this site](http://setosa.io/ev/principal-component-analysis/). Play around with the 2D data. Take 2-3 minutes.\n",
    "1. As you interact with the data, how would you describe the red line?\n",
    "2. As you interact with the data, how would you describe the green line?\n",
    "\n",
    "---\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "- We are looking for new *directions* in feature space\n",
    "- Each consecutive direction tries to maximize *remaining variance*\n",
    "- Each direction is *orthogonal* to all the others\n",
    "\n",
    "**These new *directions* are the \"principal components\", i.e. the new coordinate system for your data.**\n",
    "\n",
    "> Applying PCA to your data *transforms* your original data columns (variables) onto the new principal component axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One note:\n",
    "You will do your train/test split **before** applying PCA!\n",
    "- Otherwise, your test set would affect the transformation we apply to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>**Check:** Suppose I ran `sns.pairplot(X_train)` and `sns.pairplot(Z_train)`. Given what we know about PCA, how would they be different?</summary>\n",
    "`X_train` is the original data and `Z_train` is the transformed data. Since the columns of `Z_train` are uncorrelated, we should see no linear relationships between any of the images in that plot! (The line of best fit should be flat.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: Uncomment and run the next cell **if you have time.** This will take a few minutes to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.pairplot(X_train)\n",
    "# sns.pairplot(Z_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, like, big picture, what is PCA doing?\n",
    "Well, we're transforming our data. Specifically, we are:\n",
    "1. We are going to look at how all of the $X$ variables relate to one another, then summarize these relationships. (This is done with the **covariance matrix**.)\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important. (We will decompose our covariance matrix into its **eigenvectors**, which is a linear algebra term that allows us to understand the most important \"directions\" in our data, which are our principal components!)\n",
    "3. We can also see exactly how important each combination is, then rank these combinations. (With each eigenvector, we get an **eigenvalue**. This eigenvalue is a number that tells us how important each \"direction\" or principal component is.)\n",
    "\n",
    "Remember that one of our goals with PCA is to do dimensionality reduction (a.k.a. get rid of features).\n",
    "\n",
    "We can measure how important each principal component is using the eigenvalue, rank the columns of `Z_train` by their eigenvalues, and then drop the columns with small eigenvalues (little importance) but keep the columns with big eigenvalues (very important).\n",
    "- In `sklearn`, when transformed by PCA, the columns will already be sorted by their eigenvalues from biggest to smallest! The first column will be the most important, the second column will be the next most important, and so on.\n",
    "\n",
    "#### Capturing variance\n",
    "\n",
    "The total variance of your data gets redistributed among the principal components:\n",
    "\n",
    "$$\\text{var}(PC1) > \\text{var}(PC2) > \\text{var}(PC3) > \\cdots$$\n",
    "\n",
    "#### But how many features do we discard?\n",
    "\n",
    "A useful measure is the **proportion of explained variance**, which is calculated from the **eigenvalues**. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ \\text{explained variance of }PC_k = \\bigg(\\frac{\\text{eigenvalue of } PC_k}{\\sum_{i=1}^p\\text{eigenvalue of } PC_i}\\bigg)$$\n",
    "\n",
    "Rather than write out \"$\\text{eigenvalue of } PC_k$\", we usually just write $\\lambda_k$.\n",
    "\n",
    "If I want to calculate the proportion of explained variance by retaining $PC_1$ and $PC_2$, I would calculate this as:\n",
    "\n",
    "### $$ \\text{explained variance of } PC_1 \\text{ and } PC_2 = \\bigg(\\frac{\\lambda_1 + \\lambda_2}{\\sum_{i=1}^p \\lambda_i} \\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>**Check:** If I wanted to explain at least 99.9% of the variability in my data with principal components, what is the smallest number of principal components that I would need to keep? </summary>\n",
    "Only two!! I could keep $Z_1$ and $Z_2$, and this would explain 99.931578% of the variability in my data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# HERE IS THE BASIC PLOT\n",
    "\n",
    "# Plot the explained variance\n",
    "component_number = range(len(var_exp))\n",
    "plt.plot(component_number, var_exp, lw=3)\n",
    "\n",
    "# NOW MAKE IT LOOK PRETTY\n",
    "\n",
    "# Add horizontal lines at y=0 and y=100\n",
    "plt.axhline(y=0, linewidth=1, color='grey', ls='dashed')\n",
    "plt.axhline(y=1, linewidth=1, color='grey', ls='dashed')\n",
    "\n",
    "# Set the x and y axis limits\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-1,26])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "# Label the axes\n",
    "ax.set_ylabel('variance explained', fontsize=16)\n",
    "ax.set_xlabel('component', fontsize=16)\n",
    "\n",
    "# Make the tick labels bigger\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "# Add title\n",
    "ax.set_title('Component vs Variance Explained\\n', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare our PCA'ed performance to our original performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two assumptions that PCA makes:**\n",
    "1. **Linearity:** PCA detects and controls for linear relationships, so we assume that the data does not hold nonlinear relationships.\n",
    "    - We are using our covariance matrix to determine important \"directions,\" which is a measure of the linear relationship between observations!\n",
    "2. **Large variances define importance:** If data is spread in a direction, that direction is important! If there is little spread in a direction, that direction is not very important.\n",
    "    - That aligns with what we saw [here](http://setosa.io/ev/principal-component-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Use Cases for PCA\n",
    "- Image Processing (we'll learn other methods of dealing with these later, too)\n",
    "- 20+ Variables with High Multicollinearity\n",
    "- 100+ Variables\n",
    "- Situations where $n < p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "By the end of the lesson, students should be able to:\n",
    "1. Differentiate between feature elimination and feature extraction.\n",
    "2. Describe the PCA algorithm.\n",
    "3. Identify use cases for PCA.\n",
    "4. Implement PCA in `scikit-learn`.\n",
    "5. Calculate and interpret proportion of explained variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
