{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lab\n",
    "\n",
    "In this lab, we'll be exploring a visual proof of the universal approximation theorem and building (from scratch) a neural network that will approximate a pretty ridiculous function.\n",
    "\n",
    "Head over to [this site](http://neuralnetworksanddeeplearning.com/chap4.html) and read from the beginning of the page until the \"Many Input Variables\" section. (You do not need to read the \"Many Input Variables\" section and beyond but are certainly welcome to do so!) You'll read the introduction, the \"Two Caveats\" section, and the \"Universality with One Input and One Output\" section.\n",
    "\n",
    "Your answers to problems 1-5 should come from directly this reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**: Summarize the Universal Approximation Theorem. (Don't copy it; use your own words!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Any function can be approximated using a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**: Summarize the two caveats the author uses to describe the statement \"a neural network can compute any function.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One caveat is that this will only be an approximation\n",
    "The other is that it must be continuous(or the continuous approximation of a discontinuous function. i.e. in practice not typically an issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** For a sigmoidal activation function to closely resemble a step function, how would you describe the value of $w$? What constraints exist on the value of $b$? How do we calculate $s$? What does the value of $s$ indicate?\n",
    "\n",
    "Try playing around with the applets on the page to test how different parts of the perceptron affect the output. This should be helpful in answering the questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The value of w must be very high for it to resemble a step function\n",
    "s = -b/w, which is the position of the step function based on bias, b and weight, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: When the author wants us to approximate $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ with a neural network, the function on the applet where we manipulate the values of $h_i$ is not $f(x)$. It's a different function. What is this function, and why are we working with this one instead of $f(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function is the inverse of f(x).  Because we need to compute that in order to solve the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: The author asks you to find values of $h_i$ that make your neural network closely approximate $\\sigma^{-1}\\circ f(x)$. Record your values of $h_i$ here and your best \"average deviation\" score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "h1 = -1.2\n",
    "h2 = -1.8\n",
    "h3 = -0.2\n",
    "h4 = -1.1\n",
    "h5 = 1.2\n",
    "deviation = .40\n",
    "\n",
    "h1 = -1.4\n",
    "h2 = -1.6\n",
    "h3 = -0.4\n",
    "h4 = -1.0\n",
    "h5 = 1.2\n",
    "deviation = .38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**: Build the neural network from your work in Problem 5 here.\n",
    "\n",
    "A few things to keep in mind:\n",
    "* How many inputs are there? \n",
    "* How many outputs are there?\n",
    "* How many neurons are in the hidden layer?\n",
    "* In order to create step functions between 0 and 0.2, 0.2 and 0.4, etc., what does this suggest about the activation function in these neurons? Note that these activation functions will be different, but related.\n",
    "* What do the values of $h_i$ represent?\n",
    "\n",
    "Check out the Neural Networks I notes for an implementation in NumPy; you should be able to use this as a starting point for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* One input\n",
    "* One output\n",
    "* Two neurons in hidden layer ( i don't remember how many)\n",
    "* I _think_ the answer to this one is that the biases and weights will be differently bounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: Once you've built the neural network, use `np.linspace` to generate 1000 values of $x$ between 0 and 1 and use the `pynverse` [library](https://pypi.python.org/pypi/pynverse) to manually estimate the performance of your neural network using mean squared error.\n",
    "\n",
    "Recall that mean squared error is given by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}-y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Your $\\hat{y}$ in this case are your predicted values from your neural network for each of the $x$ that you generated using `np.linspace`. Make sure to take into account the final activation function!\n",
    "* Your $y$ values are the actual observed values of $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ for each of the $x$ that you generated using `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sigmoid activation function\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "    return 1/(1 + np.e ** -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**: Suppose you wanted to increase the performance of this neural network. How might you go about doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add more layers/neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
