{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"\\>\n",
    "\n",
    "## Regularization and Optimization for Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "_Authors: Matt Brems and Justin Pounders (but mainly Matt)_\n",
    "\n",
    "The principal topic of the day is **how to control bias/variance tradeoff in neural nets.**\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of the lesson, students should be able to:\n",
    "- Explain how L1/L2, dropout, and early stopping regularization work and implement these methods in Keras\n",
    "- Implement methods for speeding up learning\n",
    "- Describe Gradient Descent with Momentum and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>vmail_plan</th>\n",
       "      <th>vmail_message</th>\n",
       "      <th>day_mins</th>\n",
       "      <th>day_calls</th>\n",
       "      <th>day_charge</th>\n",
       "      <th>eve_mins</th>\n",
       "      <th>eve_calls</th>\n",
       "      <th>eve_charge</th>\n",
       "      <th>night_mins</th>\n",
       "      <th>night_calls</th>\n",
       "      <th>night_charge</th>\n",
       "      <th>intl_mins</th>\n",
       "      <th>intl_calls</th>\n",
       "      <th>intl_charge</th>\n",
       "      <th>custserv_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account_length  area_code intl_plan vmail_plan  vmail_message  \\\n",
       "0    KS             128        415        no        yes             25   \n",
       "1    OH             107        415        no        yes             26   \n",
       "2    NJ             137        415        no         no              0   \n",
       "3    OH              84        408       yes         no              0   \n",
       "4    OK              75        415       yes         no              0   \n",
       "\n",
       "   day_mins  day_calls  day_charge  eve_mins  eve_calls  eve_charge  \\\n",
       "0     265.1        110       45.07     197.4         99       16.78   \n",
       "1     161.6        123       27.47     195.5        103       16.62   \n",
       "2     243.4        114       41.38     121.2        110       10.30   \n",
       "3     299.4         71       50.90      61.9         88        5.26   \n",
       "4     166.7        113       28.34     148.3        122       12.61   \n",
       "\n",
       "   night_mins  night_calls  night_charge  intl_mins  intl_calls  intl_charge  \\\n",
       "0       244.7           91         11.01       10.0           3         2.70   \n",
       "1       254.4          103         11.45       13.7           3         3.70   \n",
       "2       162.6          104          7.32       12.2           5         3.29   \n",
       "3       196.9           89          8.86        6.6           7         1.78   \n",
       "4       186.9          121          8.41       10.1           3         2.73   \n",
       "\n",
       "   custserv_calls  churn  \n",
       "0               1  False  \n",
       "1               1  False  \n",
       "2               0  False  \n",
       "3               2  False  \n",
       "4               3  False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/cell_phone_churn.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3333 entries, 0 to 3332\n",
      "Data columns (total 19 columns):\n",
      "account_length    3333 non-null int64\n",
      "area_code         3333 non-null int64\n",
      "intl_plan         3333 non-null int64\n",
      "vmail_plan        3333 non-null int64\n",
      "vmail_message     3333 non-null int64\n",
      "day_mins          3333 non-null float64\n",
      "day_calls         3333 non-null int64\n",
      "day_charge        3333 non-null float64\n",
      "eve_mins          3333 non-null float64\n",
      "eve_calls         3333 non-null int64\n",
      "eve_charge        3333 non-null float64\n",
      "night_mins        3333 non-null float64\n",
      "night_calls       3333 non-null int64\n",
      "night_charge      3333 non-null float64\n",
      "intl_mins         3333 non-null float64\n",
      "intl_calls        3333 non-null int64\n",
      "intl_charge       3333 non-null float64\n",
      "custserv_calls    3333 non-null int64\n",
      "churn             3333 non-null bool\n",
      "dtypes: bool(1), float64(8), int64(10)\n",
      "memory usage: 472.0 KB\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('state', axis=1)\n",
    "data['intl_plan'] = data['intl_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "data['vmail_plan'] = data['vmail_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('churn', axis=1)\n",
    "y = data['churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple neural net to model churn\n",
    "\n",
    "Let's build this:\n",
    "\n",
    "- a dense network,\n",
    "- one input layer,\n",
    "- one hidden layer \n",
    "  - same number of nodes as input layer,\n",
    "  - ReLU activation\n",
    "- single node output (for binary classification)\n",
    "  - sigmoid activation\n",
    "  \n",
    "> **Fun fact**: If we dropped the hidden layer, this model would just be logistic regression!  Can you prove that to yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 258us/step - loss: 0.5589 - acc: 0.7903 - val_loss: 0.4900 - val_acc: 0.8513\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.4461 - acc: 0.8535 - val_loss: 0.4269 - val_acc: 0.8549\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.4005 - acc: 0.8567 - val_loss: 0.3918 - val_acc: 0.8561\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3721 - acc: 0.8575 - val_loss: 0.3674 - val_acc: 0.8585\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3513 - acc: 0.8595 - val_loss: 0.3490 - val_acc: 0.8633\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3348 - acc: 0.8671 - val_loss: 0.3345 - val_acc: 0.8633\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.3210 - acc: 0.8739 - val_loss: 0.3232 - val_acc: 0.8645\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3101 - acc: 0.8784 - val_loss: 0.3134 - val_acc: 0.8669\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3006 - acc: 0.8832 - val_loss: 0.3046 - val_acc: 0.8717\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.2911 - acc: 0.8840 - val_loss: 0.2968 - val_acc: 0.8741\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2823 - acc: 0.8904 - val_loss: 0.2888 - val_acc: 0.8765\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.2747 - acc: 0.8956 - val_loss: 0.2816 - val_acc: 0.8777\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2668 - acc: 0.9004 - val_loss: 0.2750 - val_acc: 0.8849\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.2599 - acc: 0.9032 - val_loss: 0.2689 - val_acc: 0.8861\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2537 - acc: 0.9056 - val_loss: 0.2638 - val_acc: 0.8873\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2478 - acc: 0.9124 - val_loss: 0.2593 - val_acc: 0.8897\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.2432 - acc: 0.9164 - val_loss: 0.2555 - val_acc: 0.8921\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2393 - acc: 0.9192 - val_loss: 0.2530 - val_acc: 0.8945\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2350 - acc: 0.9212 - val_loss: 0.2509 - val_acc: 0.8981\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2319 - acc: 0.9272 - val_loss: 0.2483 - val_acc: 0.8945\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.2290 - acc: 0.9276 - val_loss: 0.2463 - val_acc: 0.8957\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.2259 - acc: 0.9284 - val_loss: 0.2444 - val_acc: 0.8969\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2241 - acc: 0.9284 - val_loss: 0.2431 - val_acc: 0.8969\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2220 - acc: 0.9308 - val_loss: 0.2427 - val_acc: 0.8969\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.2195 - acc: 0.9328 - val_loss: 0.2411 - val_acc: 0.9017\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2179 - acc: 0.9328 - val_loss: 0.2396 - val_acc: 0.9041\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.2159 - acc: 0.9336 - val_loss: 0.2397 - val_acc: 0.9017\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2151 - acc: 0.9324 - val_loss: 0.2387 - val_acc: 0.9017\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2133 - acc: 0.9312 - val_loss: 0.2379 - val_acc: 0.9077\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.2117 - acc: 0.9364 - val_loss: 0.2376 - val_acc: 0.9077\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.2104 - acc: 0.9360 - val_loss: 0.2375 - val_acc: 0.9101\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.2096 - acc: 0.9344 - val_loss: 0.2371 - val_acc: 0.9077\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.2082 - acc: 0.9372 - val_loss: 0.2368 - val_acc: 0.9101\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2067 - acc: 0.9380 - val_loss: 0.2358 - val_acc: 0.9077\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2061 - acc: 0.9368 - val_loss: 0.2362 - val_acc: 0.9125\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.2052 - acc: 0.9368 - val_loss: 0.2353 - val_acc: 0.9137\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.2039 - acc: 0.9368 - val_loss: 0.2353 - val_acc: 0.9113\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.2037 - acc: 0.9380 - val_loss: 0.2353 - val_acc: 0.9125\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.2024 - acc: 0.9372 - val_loss: 0.2345 - val_acc: 0.9113\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.2018 - acc: 0.9380 - val_loss: 0.2341 - val_acc: 0.9137\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.2014 - acc: 0.9408 - val_loss: 0.2341 - val_acc: 0.9113\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.2004 - acc: 0.9396 - val_loss: 0.2339 - val_acc: 0.9149\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1997 - acc: 0.9384 - val_loss: 0.2339 - val_acc: 0.9137\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.1990 - acc: 0.9392 - val_loss: 0.2341 - val_acc: 0.9137\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1985 - acc: 0.9412 - val_loss: 0.2339 - val_acc: 0.9149\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.1984 - acc: 0.9376 - val_loss: 0.2346 - val_acc: 0.9149\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1973 - acc: 0.9404 - val_loss: 0.2341 - val_acc: 0.9125\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 50us/step - loss: 0.1966 - acc: 0.9408 - val_loss: 0.2326 - val_acc: 0.9125\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1958 - acc: 0.9412 - val_loss: 0.2325 - val_acc: 0.9161\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1957 - acc: 0.9404 - val_loss: 0.2319 - val_acc: 0.9125\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1951 - acc: 0.9404 - val_loss: 0.2332 - val_acc: 0.9125\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1950 - acc: 0.9428 - val_loss: 0.2327 - val_acc: 0.9149\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.1939 - acc: 0.9416 - val_loss: 0.2320 - val_acc: 0.9173\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.1937 - acc: 0.9416 - val_loss: 0.2322 - val_acc: 0.9161\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1936 - acc: 0.9404 - val_loss: 0.2324 - val_acc: 0.9161\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1932 - acc: 0.9400 - val_loss: 0.2327 - val_acc: 0.9185\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1928 - acc: 0.9428 - val_loss: 0.2333 - val_acc: 0.9149\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1923 - acc: 0.9416 - val_loss: 0.2328 - val_acc: 0.9149\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.1912 - acc: 0.9428 - val_loss: 0.2322 - val_acc: 0.9161\n",
      "Epoch 60/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.1908 - acc: 0.9436 - val_loss: 0.2326 - val_acc: 0.9185\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1902 - acc: 0.9420 - val_loss: 0.2337 - val_acc: 0.9173\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.1906 - acc: 0.9432 - val_loss: 0.2326 - val_acc: 0.9161\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1904 - acc: 0.9440 - val_loss: 0.2320 - val_acc: 0.9161\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1898 - acc: 0.9440 - val_loss: 0.2314 - val_acc: 0.9161\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1893 - acc: 0.9428 - val_loss: 0.2316 - val_acc: 0.9185\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1895 - acc: 0.9420 - val_loss: 0.2314 - val_acc: 0.9197\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1887 - acc: 0.9436 - val_loss: 0.2319 - val_acc: 0.9185\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1878 - acc: 0.9444 - val_loss: 0.2320 - val_acc: 0.9185\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1876 - acc: 0.9432 - val_loss: 0.2314 - val_acc: 0.9185\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.1873 - acc: 0.9424 - val_loss: 0.2311 - val_acc: 0.9185\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1868 - acc: 0.9440 - val_loss: 0.2315 - val_acc: 0.9185\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 49us/step - loss: 0.1871 - acc: 0.9444 - val_loss: 0.2315 - val_acc: 0.9185\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1866 - acc: 0.9420 - val_loss: 0.2325 - val_acc: 0.9197\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.1859 - acc: 0.9444 - val_loss: 0.2320 - val_acc: 0.9197\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.1863 - acc: 0.9444 - val_loss: 0.2315 - val_acc: 0.9233\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.1851 - acc: 0.9456 - val_loss: 0.2308 - val_acc: 0.9257\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.1847 - acc: 0.9436 - val_loss: 0.2315 - val_acc: 0.9197\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.1846 - acc: 0.9444 - val_loss: 0.2311 - val_acc: 0.9221\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1845 - acc: 0.9440 - val_loss: 0.2300 - val_acc: 0.9209\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.1839 - acc: 0.9460 - val_loss: 0.2307 - val_acc: 0.9197\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1835 - acc: 0.9436 - val_loss: 0.2299 - val_acc: 0.9209\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.1830 - acc: 0.9468 - val_loss: 0.2315 - val_acc: 0.9197\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1828 - acc: 0.9440 - val_loss: 0.2303 - val_acc: 0.9197\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1822 - acc: 0.9460 - val_loss: 0.2305 - val_acc: 0.9197\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1824 - acc: 0.9460 - val_loss: 0.2299 - val_acc: 0.9221\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1822 - acc: 0.9456 - val_loss: 0.2310 - val_acc: 0.9221\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1817 - acc: 0.9456 - val_loss: 0.2308 - val_acc: 0.9221\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1814 - acc: 0.9448 - val_loss: 0.2302 - val_acc: 0.9209\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1810 - acc: 0.9452 - val_loss: 0.2305 - val_acc: 0.9233\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1822 - acc: 0.9452 - val_loss: 0.2311 - val_acc: 0.9233\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.1807 - acc: 0.9448 - val_loss: 0.2300 - val_acc: 0.9221\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.1801 - acc: 0.9468 - val_loss: 0.2309 - val_acc: 0.9257\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1800 - acc: 0.9464 - val_loss: 0.2310 - val_acc: 0.9221\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1792 - acc: 0.9460 - val_loss: 0.2303 - val_acc: 0.9233\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.1794 - acc: 0.9460 - val_loss: 0.2314 - val_acc: 0.9221\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1792 - acc: 0.9464 - val_loss: 0.2301 - val_acc: 0.9245\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.1790 - acc: 0.9476 - val_loss: 0.2307 - val_acc: 0.9233\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 50us/step - loss: 0.1785 - acc: 0.9504 - val_loss: 0.2301 - val_acc: 0.9245\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.1784 - acc: 0.9492 - val_loss: 0.2303 - val_acc: 0.9233\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.1781 - acc: 0.9484 - val_loss: 0.2302 - val_acc: 0.9257\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "              epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2cabf3c8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2c8cc9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(test_loss, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9483793517406963, 0.9256594724220624)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['acc'][-1], history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without regularization, **val_loss: 0.1820**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='l1'></a>\n",
    "## Regularization Method 1: L1 and L2 \n",
    "---\n",
    "Just as we did with linear and logistic regression, we can use `L1` and `L2` regularization on our neural networks.\n",
    "\n",
    "Neural networks are just large combinations of linear functions that are modified using some activation function:\n",
    "\n",
    "$$z = b_0 + \\sum_{j=1}^p w_j x_j$$\n",
    "$$a = g(z)$$\n",
    "\n",
    "Where $x_j$ is one input (i.e. one observation's blood pressure, one observation's sex, etc.), $w_j$ is the weight/coefficient for that particular variable, $b_0$ is our bias, and $g$ is our activation function. If we used a sigmoid function as we would for logistic regression, $g$ would be:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "After we've done this for every node, we can then compute the loss for every node as a function of their parameters:\n",
    "$$\\text{loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i)$$\n",
    "\n",
    "This is our average loss. In a regression context, this is usually mean squared error; in a classification context this might be categorical cross-entropy or something else. This would be our loss function *without regularization*.\n",
    "\n",
    "We'll then implement gradient descent:\n",
    "\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{loss}}{\\partial w_j}$$\n",
    "\n",
    "where $\\alpha$ is our learning rate and $\\frac{\\partial \\text{loss}}{\\partial w_j}$ represents the partial derivative of our loss function with respect to our weight $w_j$.\n",
    "\n",
    "This is how we implement gradient descent **without regularization**.\n",
    "\n",
    "#### So, how do we implement gradient descent with `L1` or `L2` regularization?\n",
    "\n",
    "> We just change the loss function to add a penalty! If we want to add a penalty term, we do the **exact same thing** we did with linear or logistic regression:\n",
    "\n",
    "$$\\text{L2 regularized loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||w_{[l]}||^2$$\n",
    "\n",
    "Now, $$\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_{[l]}} = \\frac{\\partial \\text{loss}}{\\partial w_j} + \\frac{\\lambda}{m}w_j$$\n",
    "and\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_j}$$\n",
    "\n",
    "In this example we used `L2` regularization, although `L1` works in the same way. You may see `L2` regularization referred to as \"*weight decay*.\"\n",
    "\n",
    "**Practical Note:** According to Andrew Ng, `L2` (as opposed to `L1`) is generally used for regularizing neural networks and it's rare to find `L1`.\n",
    "\n",
    "As before, $\\lambda$ is a hyperparameter to be selected by constructing multiple networks and identifying which value performs the best.\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, our weights get closer and closer to 0 (just like when we regularized our linear models before).\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, if we're using the `sigmoid` or `tanh` activation functions, we force our weights to stay in that \"linear\" region in the middle. This speeds up the learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 294us/step - loss: 0.7295 - acc: 0.7663 - val_loss: 0.6446 - val_acc: 0.8309\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.5887 - acc: 0.8563 - val_loss: 0.5588 - val_acc: 0.8525\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.5275 - acc: 0.8579 - val_loss: 0.5111 - val_acc: 0.8537\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.4891 - acc: 0.8607 - val_loss: 0.4788 - val_acc: 0.8537\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.4607 - acc: 0.8631 - val_loss: 0.4536 - val_acc: 0.8561\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.4383 - acc: 0.8667 - val_loss: 0.4338 - val_acc: 0.8609\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.4202 - acc: 0.8671 - val_loss: 0.4181 - val_acc: 0.8621\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.4052 - acc: 0.8707 - val_loss: 0.4051 - val_acc: 0.8597\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3928 - acc: 0.8719 - val_loss: 0.3935 - val_acc: 0.8633\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3814 - acc: 0.8756 - val_loss: 0.3829 - val_acc: 0.8609\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3720 - acc: 0.8776 - val_loss: 0.3758 - val_acc: 0.8633\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3640 - acc: 0.8772 - val_loss: 0.3682 - val_acc: 0.8669\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3572 - acc: 0.8816 - val_loss: 0.3619 - val_acc: 0.8693\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3510 - acc: 0.8844 - val_loss: 0.3566 - val_acc: 0.8693\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3464 - acc: 0.8844 - val_loss: 0.3527 - val_acc: 0.8729\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3422 - acc: 0.8876 - val_loss: 0.3486 - val_acc: 0.8765\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3387 - acc: 0.8868 - val_loss: 0.3450 - val_acc: 0.8813\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3356 - acc: 0.8912 - val_loss: 0.3410 - val_acc: 0.8801\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3325 - acc: 0.8916 - val_loss: 0.3388 - val_acc: 0.8837\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3307 - acc: 0.8900 - val_loss: 0.3360 - val_acc: 0.8921\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3278 - acc: 0.8964 - val_loss: 0.3345 - val_acc: 0.8849\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3264 - acc: 0.8948 - val_loss: 0.3323 - val_acc: 0.8921\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3247 - acc: 0.8992 - val_loss: 0.3312 - val_acc: 0.8909\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3237 - acc: 0.8988 - val_loss: 0.3291 - val_acc: 0.8909\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3225 - acc: 0.8988 - val_loss: 0.3276 - val_acc: 0.8909\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3213 - acc: 0.8992 - val_loss: 0.3267 - val_acc: 0.8933\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3203 - acc: 0.9000 - val_loss: 0.3258 - val_acc: 0.8921\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3192 - acc: 0.9004 - val_loss: 0.3248 - val_acc: 0.8969\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3185 - acc: 0.9008 - val_loss: 0.3249 - val_acc: 0.8933\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3177 - acc: 0.8992 - val_loss: 0.3233 - val_acc: 0.8957\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3175 - acc: 0.9020 - val_loss: 0.3224 - val_acc: 0.8993\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3165 - acc: 0.9048 - val_loss: 0.3212 - val_acc: 0.8993\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3163 - acc: 0.9024 - val_loss: 0.3216 - val_acc: 0.8957\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3165 - acc: 0.9016 - val_loss: 0.3207 - val_acc: 0.9017\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3152 - acc: 0.9060 - val_loss: 0.3206 - val_acc: 0.9005\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3154 - acc: 0.9020 - val_loss: 0.3199 - val_acc: 0.9017\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3146 - acc: 0.9060 - val_loss: 0.3187 - val_acc: 0.9017\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3143 - acc: 0.9064 - val_loss: 0.3196 - val_acc: 0.9005\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3143 - acc: 0.9028 - val_loss: 0.3184 - val_acc: 0.9017\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3140 - acc: 0.9064 - val_loss: 0.3179 - val_acc: 0.9029\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3133 - acc: 0.9088 - val_loss: 0.3177 - val_acc: 0.9041\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3131 - acc: 0.9052 - val_loss: 0.3179 - val_acc: 0.9041\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3133 - acc: 0.9140 - val_loss: 0.3193 - val_acc: 0.8993\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.3129 - acc: 0.9040 - val_loss: 0.3175 - val_acc: 0.9017\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3133 - acc: 0.9048 - val_loss: 0.3173 - val_acc: 0.9041\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3121 - acc: 0.9076 - val_loss: 0.3167 - val_acc: 0.9041\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3128 - acc: 0.9036 - val_loss: 0.3166 - val_acc: 0.9017\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3122 - acc: 0.9060 - val_loss: 0.3160 - val_acc: 0.9041\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3121 - acc: 0.9068 - val_loss: 0.3160 - val_acc: 0.9041\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3121 - acc: 0.9048 - val_loss: 0.3152 - val_acc: 0.9029\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3115 - acc: 0.9072 - val_loss: 0.3152 - val_acc: 0.9041\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3115 - acc: 0.9052 - val_loss: 0.3157 - val_acc: 0.9041\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3114 - acc: 0.9076 - val_loss: 0.3152 - val_acc: 0.9041\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3117 - acc: 0.9040 - val_loss: 0.3159 - val_acc: 0.9029\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3112 - acc: 0.9080 - val_loss: 0.3157 - val_acc: 0.9017\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3119 - acc: 0.9036 - val_loss: 0.3155 - val_acc: 0.9029\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3109 - acc: 0.9068 - val_loss: 0.3157 - val_acc: 0.9029\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3113 - acc: 0.9048 - val_loss: 0.3150 - val_acc: 0.9029\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3109 - acc: 0.9068 - val_loss: 0.3147 - val_acc: 0.9029\n",
      "Epoch 60/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3105 - acc: 0.9068 - val_loss: 0.3148 - val_acc: 0.9041\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3108 - acc: 0.9072 - val_loss: 0.3143 - val_acc: 0.9029\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3109 - acc: 0.9040 - val_loss: 0.3146 - val_acc: 0.9041\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3110 - acc: 0.9060 - val_loss: 0.3154 - val_acc: 0.9017\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3105 - acc: 0.9048 - val_loss: 0.3146 - val_acc: 0.9065\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3110 - acc: 0.9088 - val_loss: 0.3138 - val_acc: 0.9029\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3106 - acc: 0.9064 - val_loss: 0.3138 - val_acc: 0.9041\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3106 - acc: 0.9044 - val_loss: 0.3142 - val_acc: 0.9017\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3103 - acc: 0.9072 - val_loss: 0.3146 - val_acc: 0.9029\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3105 - acc: 0.9040 - val_loss: 0.3137 - val_acc: 0.9029\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3110 - acc: 0.9092 - val_loss: 0.3142 - val_acc: 0.9053\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3103 - acc: 0.9096 - val_loss: 0.3143 - val_acc: 0.9029\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3102 - acc: 0.9056 - val_loss: 0.3135 - val_acc: 0.9041\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3101 - acc: 0.9064 - val_loss: 0.3138 - val_acc: 0.9053\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3106 - acc: 0.9068 - val_loss: 0.3139 - val_acc: 0.9041\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.3101 - acc: 0.9064 - val_loss: 0.3136 - val_acc: 0.9029\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3103 - acc: 0.9076 - val_loss: 0.3140 - val_acc: 0.9041\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3101 - acc: 0.9056 - val_loss: 0.3142 - val_acc: 0.9041\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3098 - acc: 0.9052 - val_loss: 0.3132 - val_acc: 0.9041\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3098 - acc: 0.9052 - val_loss: 0.3131 - val_acc: 0.9053\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3104 - acc: 0.9080 - val_loss: 0.3135 - val_acc: 0.9053\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3099 - acc: 0.9052 - val_loss: 0.3140 - val_acc: 0.9053\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3094 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9041\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3102 - acc: 0.9044 - val_loss: 0.3137 - val_acc: 0.9041\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 51us/step - loss: 0.3095 - acc: 0.9044 - val_loss: 0.3133 - val_acc: 0.9041\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 50us/step - loss: 0.3106 - acc: 0.9072 - val_loss: 0.3133 - val_acc: 0.9017\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 50us/step - loss: 0.3096 - acc: 0.9120 - val_loss: 0.3138 - val_acc: 0.9041\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3096 - acc: 0.9064 - val_loss: 0.3131 - val_acc: 0.9029\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3097 - acc: 0.9032 - val_loss: 0.3127 - val_acc: 0.9017\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3095 - acc: 0.9096 - val_loss: 0.3129 - val_acc: 0.9017\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3097 - acc: 0.9072 - val_loss: 0.3140 - val_acc: 0.9029\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3096 - acc: 0.9060 - val_loss: 0.3134 - val_acc: 0.9029\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3097 - acc: 0.9088 - val_loss: 0.3128 - val_acc: 0.9053\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3098 - acc: 0.9120 - val_loss: 0.3131 - val_acc: 0.9017\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3097 - acc: 0.9084 - val_loss: 0.3134 - val_acc: 0.9053\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3096 - acc: 0.9044 - val_loss: 0.3128 - val_acc: 0.9029\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3091 - acc: 0.9052 - val_loss: 0.3128 - val_acc: 0.9053\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3096 - acc: 0.9056 - val_loss: 0.3126 - val_acc: 0.9053\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3096 - acc: 0.9068 - val_loss: 0.3128 - val_acc: 0.9041\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3094 - acc: 0.9088 - val_loss: 0.3128 - val_acc: 0.9041\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3096 - acc: 0.9048 - val_loss: 0.3122 - val_acc: 0.9041\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_2.add(Dense(n_hidden, input_dim=n_input, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_2.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_2 = model_2.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "              epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2d8a2f60>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2bda0eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_2 = history_2.history['loss']\n",
    "test_loss_2 = history_2.history['val_loss']\n",
    "plt.plot(train_loss_2, label='Training loss')\n",
    "plt.plot(test_loss_2, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `L2` regularization and $\\lambda = 0.01$, **val_loss: 0.2075**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9047619047619048, 0.9040767386091128)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_2.history['acc'][-1], history_2.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Regularization Documentation](https://keras.io/regularizers/)\n",
    "- [Kernel vs. Activity Regularizers](https://github.com/keras-team/keras/issues/3236)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Implementation in Tensorflow](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/regularizers)\n",
    "- [Example in Tensorflow](http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## Regularization Method 2: Dropout\n",
    "---\n",
    "There's another method of regularizing our terms that is specifically designed for neural networks, called **dropout regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we've constructed a neural network. We've decided on the number of layers we want and the number of nodes in each layer. (We might say that we've decided on the **topology** or **structure** of our network.)\n",
    "\n",
    "![](../assets/original_nn.jpeg)\n",
    "\n",
    "However, a densely connected network like this will almost certainly overfit. Our network is learning a parameter for every single connection.\n",
    "\n",
    "> We can overcome this by using **dropout regularization**. \n",
    "\n",
    "In dropout regularization, we randomly **drop** units (nodes) in our neural network ***during our training phase only***. We assign a probability of each node disappearing. Then, we essentially perform a coinflip for every node to turn that node \"on\" or \"off.\"\n",
    "\n",
    "Let's go through an example to illustrate this: For simplicity, we'll say we've assigned a 0.5 probability of keeping to every node in the network above. Then, for every node, we flip a coin, and if the coin lands on heads, the node remains, if it lands on tails, the node disappears. After we've done this for every node, we're left with a new network that looks something like this:\n",
    "\n",
    "![](./assets/after_dropout.jpeg)\n",
    "\n",
    "<!--\n",
    "Image sources: https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\n",
    "Also, it seems, this site: http://cs231n.github.io/neural-networks-2/\n",
    "-->\n",
    "\n",
    "Let's explicitly lay out the general workflow you would follow:\n",
    "\n",
    "1. Specify the **topology** of your neural network.\n",
    "2. Initialize your weights and biases.\n",
    "3. Specify the \"keeping probabilities\" for every node. (Generally, we'll assign the same probability to all nodes in each layer and usually the same probability to all hidden layers.)\n",
    "4. Perform a \"coin flip\" for each node and drop out the chosen nodes.\n",
    "5. Run through one epoch of training.\n",
    "6. Repeat steps 4 and 5 for each epoch of training.\n",
    "\n",
    "**Check:** If I drop out a node during one of my epochs, does it disappear from my final network?\n",
    "\n",
    "#### So, what does this do?\n",
    "<!-- <br/> -->\n",
    "The intuition behind dropout is that, since each node has a probability of disappearing at any time, the neural network is disincentivized from allocating too much power to any one weight. It has a similar effect as imposing an L2 penalty: the magnitude of our weights shrinks.\n",
    "\n",
    "**Check:** What might be some potential problems with doing this?\n",
    "\n",
    "<!--\n",
    "expected values of nodes changes; induces bias\n",
    "-->\n",
    "\n",
    "#### Inverted Dropout\n",
    "\n",
    "In order to avoid any issues with the expected values of our nodes changing, we adjust our results accordingly by a method called **inverted dropout**.\n",
    "\n",
    "If we have a hidden layer with 100 nodes and each node has a 80% probability of being \"staying turned on,\" we only have 80% of those inputs to our node. As a result, we expect that the combined input to our node $z = b_0 + \\sum_{i=1}^pw_ix_i$ will be off by about 20%. (Those interested in probability and research might note that the Binomial distribution is a very convenient model for neural networks and dropout.)\n",
    "\n",
    "When using inverted dropout, we adjust $z$ by the \"keeping probability.\"\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_{original} &=& b_0 + \\sum_{i=1}^pw_ix_i \\\\\n",
    "\\Rightarrow z_{dropout} &=& b_0 + \\sum_{i\\in\\{included\\_nodes\\}}w_ix_i \\\\\n",
    "\\Rightarrow z_{inverted\\_dropout} &:=& z_{dropout} / 0.8 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "#### Test time:\n",
    "\n",
    "So we've now run through every epoch of our training phase and we're ready to apply our neural network to our validation or testing data. Are we going to apply dropout to this data as well?\n",
    "\n",
    "**NO.**\n",
    "\n",
    "#### Best practices:\n",
    "\n",
    "- Don't set any keeping probabilities for layers you where you don't want to drop any nodes. (What might be examples of these layers?)\n",
    "<!--\n",
    "Input and output layers\n",
    "-->\n",
    "- You'll generally want to specify a single keeping probability and all the layers on which you want to apply dropout, instead of specifying different keeping probabilities for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 317us/step - loss: 0.8433 - acc: 0.6279 - val_loss: 0.4810 - val_acc: 0.7998\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.6753 - acc: 0.7015 - val_loss: 0.4173 - val_acc: 0.8489\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.6224 - acc: 0.7359 - val_loss: 0.3922 - val_acc: 0.8513\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.5473 - acc: 0.7815 - val_loss: 0.3788 - val_acc: 0.8585\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.5166 - acc: 0.8099 - val_loss: 0.3712 - val_acc: 0.8585\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.4958 - acc: 0.8155 - val_loss: 0.3655 - val_acc: 0.8573\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.4858 - acc: 0.8307 - val_loss: 0.3616 - val_acc: 0.8573\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.4635 - acc: 0.8423 - val_loss: 0.3577 - val_acc: 0.8561\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.4473 - acc: 0.8427 - val_loss: 0.3534 - val_acc: 0.8549\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.4438 - acc: 0.8507 - val_loss: 0.3510 - val_acc: 0.8549\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.4422 - acc: 0.8571 - val_loss: 0.3483 - val_acc: 0.8549\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.4172 - acc: 0.8555 - val_loss: 0.3461 - val_acc: 0.8549\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.4186 - acc: 0.8531 - val_loss: 0.3434 - val_acc: 0.8549\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.4085 - acc: 0.8583 - val_loss: 0.3414 - val_acc: 0.8549\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.4070 - acc: 0.8607 - val_loss: 0.3396 - val_acc: 0.8549\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3921 - acc: 0.8591 - val_loss: 0.3363 - val_acc: 0.8549\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 88us/step - loss: 0.3948 - acc: 0.8575 - val_loss: 0.3332 - val_acc: 0.8549\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 90us/step - loss: 0.3973 - acc: 0.8603 - val_loss: 0.3316 - val_acc: 0.8549\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3888 - acc: 0.8631 - val_loss: 0.3293 - val_acc: 0.8549\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3897 - acc: 0.8599 - val_loss: 0.3279 - val_acc: 0.8549\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.3778 - acc: 0.8647 - val_loss: 0.3260 - val_acc: 0.8549\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3903 - acc: 0.8583 - val_loss: 0.3246 - val_acc: 0.8549\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3764 - acc: 0.8687 - val_loss: 0.3229 - val_acc: 0.8549\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3641 - acc: 0.8667 - val_loss: 0.3205 - val_acc: 0.8561\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.3668 - acc: 0.8659 - val_loss: 0.3185 - val_acc: 0.8573\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3594 - acc: 0.8683 - val_loss: 0.3161 - val_acc: 0.8561\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3582 - acc: 0.8719 - val_loss: 0.3132 - val_acc: 0.8597\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3525 - acc: 0.8723 - val_loss: 0.3104 - val_acc: 0.8621\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3621 - acc: 0.8671 - val_loss: 0.3110 - val_acc: 0.8585\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3558 - acc: 0.8679 - val_loss: 0.3119 - val_acc: 0.8573\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3527 - acc: 0.8695 - val_loss: 0.3108 - val_acc: 0.8585\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3410 - acc: 0.8707 - val_loss: 0.3084 - val_acc: 0.8609\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3522 - acc: 0.8687 - val_loss: 0.3070 - val_acc: 0.8609\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3522 - acc: 0.8739 - val_loss: 0.3052 - val_acc: 0.8633\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3490 - acc: 0.8695 - val_loss: 0.3048 - val_acc: 0.8645\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3425 - acc: 0.8723 - val_loss: 0.3032 - val_acc: 0.8645\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3391 - acc: 0.8715 - val_loss: 0.3017 - val_acc: 0.8657\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3335 - acc: 0.8780 - val_loss: 0.3001 - val_acc: 0.8681\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3408 - acc: 0.8731 - val_loss: 0.2983 - val_acc: 0.8693\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3388 - acc: 0.8727 - val_loss: 0.2982 - val_acc: 0.8693\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3332 - acc: 0.8824 - val_loss: 0.2976 - val_acc: 0.8693\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3436 - acc: 0.8671 - val_loss: 0.2987 - val_acc: 0.8681\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3365 - acc: 0.8719 - val_loss: 0.2964 - val_acc: 0.8693\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3312 - acc: 0.8784 - val_loss: 0.2953 - val_acc: 0.8693\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3345 - acc: 0.8752 - val_loss: 0.2932 - val_acc: 0.8705\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3332 - acc: 0.8764 - val_loss: 0.2930 - val_acc: 0.8705\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3206 - acc: 0.8820 - val_loss: 0.2904 - val_acc: 0.8729\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3412 - acc: 0.8739 - val_loss: 0.2896 - val_acc: 0.8765\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3221 - acc: 0.8780 - val_loss: 0.2896 - val_acc: 0.8741\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3298 - acc: 0.8784 - val_loss: 0.2885 - val_acc: 0.8765\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3132 - acc: 0.8792 - val_loss: 0.2865 - val_acc: 0.8753\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3270 - acc: 0.8772 - val_loss: 0.2861 - val_acc: 0.8717\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3274 - acc: 0.8735 - val_loss: 0.2867 - val_acc: 0.8717\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3261 - acc: 0.8788 - val_loss: 0.2865 - val_acc: 0.8705\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3350 - acc: 0.8735 - val_loss: 0.2854 - val_acc: 0.8729\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3227 - acc: 0.8816 - val_loss: 0.2831 - val_acc: 0.8765\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3177 - acc: 0.8864 - val_loss: 0.2812 - val_acc: 0.8777\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3217 - acc: 0.8812 - val_loss: 0.2809 - val_acc: 0.8777\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.3227 - acc: 0.8784 - val_loss: 0.2806 - val_acc: 0.8777\n",
      "Epoch 60/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3221 - acc: 0.8836 - val_loss: 0.2819 - val_acc: 0.8765\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3307 - acc: 0.8776 - val_loss: 0.2804 - val_acc: 0.8789\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3124 - acc: 0.8836 - val_loss: 0.2800 - val_acc: 0.8765\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3218 - acc: 0.8788 - val_loss: 0.2798 - val_acc: 0.8789\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3088 - acc: 0.8880 - val_loss: 0.2783 - val_acc: 0.8777\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3099 - acc: 0.8860 - val_loss: 0.2762 - val_acc: 0.8825\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3177 - acc: 0.8828 - val_loss: 0.2774 - val_acc: 0.8801\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3154 - acc: 0.8840 - val_loss: 0.2764 - val_acc: 0.8801\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3114 - acc: 0.8860 - val_loss: 0.2774 - val_acc: 0.8801\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3256 - acc: 0.8796 - val_loss: 0.2771 - val_acc: 0.8777\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3161 - acc: 0.8864 - val_loss: 0.2748 - val_acc: 0.8813\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3103 - acc: 0.8800 - val_loss: 0.2752 - val_acc: 0.8801\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3179 - acc: 0.8836 - val_loss: 0.2746 - val_acc: 0.8825\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3127 - acc: 0.8868 - val_loss: 0.2736 - val_acc: 0.8825\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3043 - acc: 0.8888 - val_loss: 0.2730 - val_acc: 0.8825\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 54us/step - loss: 0.3221 - acc: 0.8840 - val_loss: 0.2737 - val_acc: 0.8837\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3150 - acc: 0.8872 - val_loss: 0.2742 - val_acc: 0.8837\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3104 - acc: 0.8844 - val_loss: 0.2731 - val_acc: 0.8849\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3191 - acc: 0.8844 - val_loss: 0.2734 - val_acc: 0.8825\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3135 - acc: 0.8860 - val_loss: 0.2720 - val_acc: 0.8801\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3257 - acc: 0.8808 - val_loss: 0.2734 - val_acc: 0.8813\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3083 - acc: 0.8872 - val_loss: 0.2733 - val_acc: 0.8825\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3126 - acc: 0.8856 - val_loss: 0.2731 - val_acc: 0.8861\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3216 - acc: 0.8824 - val_loss: 0.2725 - val_acc: 0.8861\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3039 - acc: 0.8876 - val_loss: 0.2705 - val_acc: 0.8885\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3050 - acc: 0.8856 - val_loss: 0.2704 - val_acc: 0.8885\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3208 - acc: 0.8808 - val_loss: 0.2715 - val_acc: 0.8849\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3184 - acc: 0.8816 - val_loss: 0.2735 - val_acc: 0.8825\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 56us/step - loss: 0.3152 - acc: 0.8856 - val_loss: 0.2704 - val_acc: 0.8873\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3010 - acc: 0.8936 - val_loss: 0.2700 - val_acc: 0.8885\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3004 - acc: 0.8880 - val_loss: 0.2670 - val_acc: 0.8885\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 52us/step - loss: 0.3112 - acc: 0.8844 - val_loss: 0.2675 - val_acc: 0.8885\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3012 - acc: 0.8908 - val_loss: 0.2661 - val_acc: 0.8909\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3043 - acc: 0.8948 - val_loss: 0.2634 - val_acc: 0.8945\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3004 - acc: 0.8956 - val_loss: 0.2651 - val_acc: 0.8897\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3145 - acc: 0.8888 - val_loss: 0.2651 - val_acc: 0.8897\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3261 - acc: 0.8852 - val_loss: 0.2657 - val_acc: 0.8909\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3164 - acc: 0.8884 - val_loss: 0.2647 - val_acc: 0.8981\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.3135 - acc: 0.8848 - val_loss: 0.2659 - val_acc: 0.8945\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.3068 - acc: 0.8904 - val_loss: 0.2662 - val_acc: 0.8909\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3071 - acc: 0.8916 - val_loss: 0.2645 - val_acc: 0.8957\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_3.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_3.add(Dropout(0.8)) # refers to nodes in the first hidden layer\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_3 = model_3.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "              epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2df22908>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2d63c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_3 = history_3.history['loss']\n",
    "test_loss_3 = history_3.history['val_loss']\n",
    "plt.plot(train_loss_3, label='Training loss')\n",
    "plt.plot(test_loss_3, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Dropout, **val_loss: 0.1612**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8915566226490597, 0.89568345323741)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_3.history['acc'][-1], history_3.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Dropout Documentation](https://keras.io/layers/core/#dropout)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "- [List of examples in Tensorflow](https://programtalk.com/python-examples/tensorflow.nn.dropout/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopping'></a>\n",
    "## Regularization Method 3: Early Stopping\n",
    "---\n",
    "The third method of regularization that we'll discuss today is called early stopping.\n",
    "</br>\n",
    "If we run though all our epochs of training and plot both our training and validation error, we'll typically see something like this:\n",
    "\n",
    "![](./assets/train-val-error-reduced.png)\n",
    "*source: [Prechelt, 1997](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) *\n",
    "\n",
    "**Check:** What is happening in this plot?\n",
    "\n",
    "Early stopping does exactly what its name implies: it stop the training process early. Instead of continuing training through every epoch, once the validation error begins to increase, our algorithm stops because it has (in theory) found the minimum for the validation loss.\n",
    "\n",
    "This might seem like a simple and robust solution to overfitting, but it can run into problems.\n",
    "\n",
    "<details>\n",
    "![](../assets/validation-error-real.png)\n",
    "</details>\n",
    "\n",
    "There is debate over how often this problem occurs. You can generally plot both the training and validation loss, see if you're getting multiple optima. If you are, there are multiple suggested techniques to combat this problem in the [paper reference above](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 338us/step - loss: 0.5955 - acc: 0.6695 - val_loss: 0.4891 - val_acc: 0.8225\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.4259 - acc: 0.8471 - val_loss: 0.3948 - val_acc: 0.8561\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 53us/step - loss: 0.3692 - acc: 0.8603 - val_loss: 0.3582 - val_acc: 0.8573\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.3429 - acc: 0.8639 - val_loss: 0.3391 - val_acc: 0.8633\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 55us/step - loss: 0.3254 - acc: 0.8711 - val_loss: 0.3252 - val_acc: 0.8681\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3121 - acc: 0.8739 - val_loss: 0.3144 - val_acc: 0.8693\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.3019 - acc: 0.8776 - val_loss: 0.3063 - val_acc: 0.8705\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.2927 - acc: 0.8852 - val_loss: 0.2999 - val_acc: 0.8765\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2847 - acc: 0.8856 - val_loss: 0.2942 - val_acc: 0.8789\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.2773 - acc: 0.8904 - val_loss: 0.2885 - val_acc: 0.8789\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2700 - acc: 0.8972 - val_loss: 0.2824 - val_acc: 0.8825\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 59us/step - loss: 0.2632 - acc: 0.9016 - val_loss: 0.2780 - val_acc: 0.8825\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 57us/step - loss: 0.2569 - acc: 0.9024 - val_loss: 0.2732 - val_acc: 0.8873\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2509 - acc: 0.9036 - val_loss: 0.2691 - val_acc: 0.8909\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2455 - acc: 0.9068 - val_loss: 0.2655 - val_acc: 0.8933\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.2409 - acc: 0.9144 - val_loss: 0.2620 - val_acc: 0.8945\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.2366 - acc: 0.9140 - val_loss: 0.2596 - val_acc: 0.8957\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.2334 - acc: 0.9160 - val_loss: 0.2577 - val_acc: 0.8945\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.2302 - acc: 0.9188 - val_loss: 0.2555 - val_acc: 0.8981\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2274 - acc: 0.9208 - val_loss: 0.2537 - val_acc: 0.8981\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 58us/step - loss: 0.2249 - acc: 0.9200 - val_loss: 0.2527 - val_acc: 0.8993\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2228 - acc: 0.9260 - val_loss: 0.2514 - val_acc: 0.9005\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2204 - acc: 0.9252 - val_loss: 0.2504 - val_acc: 0.9029\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2195 - acc: 0.9236 - val_loss: 0.2507 - val_acc: 0.9005\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_4 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_4.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "history_4 = model_4.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                        epochs=100, batch_size=None,\n",
    "                        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2ef77eb8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e5e3588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_4 = history_4.history['loss']\n",
    "test_loss_4 = history_4.history['val_loss']\n",
    "plt.plot(train_loss_4, label='Training loss')\n",
    "plt.plot(test_loss_4, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With early stopping, **val_loss: 0.1210**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9343737494997999, 0.9148681055155875)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_4.history['acc'][-1], history_4.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras EarlyStopping Documentation](https://keras.io/callbacks/#earlystopping)\n",
    "- [Keras EarlyStopping Example](http://parneetk.github.io/blog/neural-networks-in-keras/)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow.Keras.callbacks.EarlyStopping Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "---\n",
    "Now we're going to talk about ways to speed up the process of optimization. Surprisingly, local optima are not often problems with neural networks; there's a much larger issue with \"plateaus,\" or areas where the derivative is approximately 0. This makes learning much slower.\n",
    "\n",
    "## Quick Fixes:\n",
    "1. **Feature Scaling:** As we've discussed before, feature scaling can speed up the process of gradient descent. Because gradient descent works \"geometrically,\" the scales of our $w$ values have a large impact on how quickly our parameters converge to the true value. Since we're working with many parameters (often 1,000 or more), scaling our features will speed up optimization.\n",
    "\n",
    "2. **Mini-batch Gradient Descent:** In `sklearn`, we use `.fit()` to estimate the parameters in our model. We do the same in neural networks, but if we're working with a very large data set (as is common in neural networks), passing data through our network will cause learning to be slow. By specifying a `batch_size` within the `.fit()` method, we can expedite our learning.\n",
    "    - `batch_size`: Integer or `None`. Number of samples per gradient update. If unspecified, it will default to 32.\n",
    "    \n",
    "**Note:** Mini batches will usually be a power of 2 (32, 64, 128, 256, 512) due to the fact that computers operate in base 2.\n",
    "\n",
    "- [Overview of the three types of gradient descent: batch, stochastic, mini-batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='momentum'></a>\n",
    "## Gradient Descent with Momentum\n",
    "\n",
    "One problem we notice with mini-batch gradient descent compared to batch gradient descent is that it tends to oscillate more, due to the fact that at each iteration it is being fed less training data than batch gradient descent.\n",
    "\n",
    "![](./assets/grad-descent.png)\n",
    "![](./assets/mini-grad-descent.png)\n",
    "\n",
    "*[source](http://pengcheng.tech/2017/09/28/gradient-descent-momentum-and-adam/)*\n",
    "\n",
    "Ideally, we'd like to take advantage of the speed and memory efficiency of mini-batch gradient descent, without the oscillations. We can use the idea of **momentum** to help us out here. (Momentum is based on [exponentially weighted moving averages](https://www.compose.com/articles/metrics-maven-calculating-an-exponentially-weighted-moving-average-in-postgresql/), which causes our oscillations to largely cancel each other out.) \n",
    "\n",
    "Typically, when updating our parameters, we'll follow this formula:\n",
    "\n",
    "$$W = W -\\alpha\\frac{\\partial \\text{loss}}{\\partial W}$$  \n",
    "$$b = b - \\alpha\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "We're going to modify this formula to this form:\n",
    "\n",
    "$$W = W -\\alpha V_{\\partial W}$$  \n",
    "$$b = b - \\alpha V_{\\partial b}$$\n",
    "\n",
    "Where \n",
    "$$V_{\\partial W} = \\beta V_{\\partial W - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial W}$$\n",
    "and\n",
    "$$V_{\\partial b} = \\beta V_{\\partial b - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "When implementing gradient descent with momentum, you'll have two hyperparameters, $\\alpha$ and $\\beta$. From a practical point of view, $\\beta$ is typically $0.9$, but you can test out other values if you'd like.\n",
    "\n",
    "### ADAM Optimization\n",
    "\n",
    "The optimization algorithm you'll likely use when implementing your feed-forward neural network is called ADAM (Adaptive Moment Estimation). It is a combination of gradient descent with momentum and another optimization method called RMSProp (Root Mean Square Propagation). For the sake of this lesson, we won't cover how ADAM works, but with gradient descent with momentum as building block, the ADAM optimization is not too far off, as it largely relies on the concept of momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "One method of minimizing the risk of overfitting is to gather more data. While this is usually very costly, we may sometimes be able to take our existing data to generate substantially more data.\n",
    "- Images: Reflect, crop, random rotations or distortions, adjust lighting.\n",
    "    - [The Effectiveness of Data Augmentation in Image Classification using Deep Learning](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    "- Non-Images: SMOTE (Synthetic Minority Over-Sampling Technique)\n",
    "    - [SMOTE Paper](http://www.jair.org/media/953/live-953-2037-jair.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "Today, we learned about three different methods of regularizing our neural networks: Regularization, dropout, and early stopping.\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "As promised, managing bias and variance takes a lot of our attention. If our bias or variance are high, it's likely that our model isn't performing as well as it could.\n",
    "\n",
    "A workflow for how you should address this (in the context of neural networks and beyond) is as follows:\n",
    "\n",
    "- Do we have high bias? (i.e. are we performing poorly on our training set?)\n",
    "    - If so:\n",
    "        - let's build a more complex model / bigger network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's train longer!\n",
    "- Do we have high variance? (i.e. are we performing poorly on our test/holdout set?)\n",
    "    - If so:\n",
    "        - let's gather more data!\n",
    "            - Usually very difficult, but we should use \"data augmentation\" if we can!\n",
    "        - let's build a simpler model / smaller network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's regularize!\n",
    "    - Once we're satisfied, return to the bias question and repeat.\n",
    "    \n",
    "**Note:** Before deep learning, most tools for handling high bias or high variance adversely affected the other. However, depending on the amount of data we have and how complex our network is, it's often the case that we can drastically reduce variance with out affecting bias.\n",
    "\n",
    "We also learned about Mini-batch Gradient Descent and Gradient Descent with Momentum. You will almost always work with mini batches. And when optimizing, you will generally use a method called Adam Optimization that is built on a combination of Gradient Descent with Momentum and another optimization technique called RMSProp.\n",
    "\n",
    "We also briefly discussed data augmentation as a method for increasing our sample sie.\n",
    "\n",
    "<a id='references'></a>\n",
    "## References and Resources:\n",
    "\n",
    "- [DeepLearning.ai](https://www.deeplearning.ai/), Andrew Ng's Coursera course on Deep Learning\n",
    "  - The videos from this course are on a [YouTube Channel](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/featured)   \n",
    "<br>\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/), textbook written by Ian Goodfellow, creator of Generative Adversarial Networks (GANs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
