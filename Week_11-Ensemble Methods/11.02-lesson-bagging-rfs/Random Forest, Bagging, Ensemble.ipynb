{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstraping, Bagging, Boosting, Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ensemble Methods\n",
    "We can list out the different types of models we've built thus far:\n",
    "- Linear Regression\n",
    "- Generalized Linear Models\n",
    "    - Logistic Regression\n",
    "    - Multinomial Logistic Regression\n",
    "    - Poisson Regression\n",
    "- $k$-Nearest Neighbors\n",
    "- Naive Bayes Classification\n",
    "\n",
    "If we want to use any of these models, we follow the same type of process.\n",
    "1. Based on our problem, we identify which model to use. (Is our problem classification or regression? Do we want an interpretable model?)\n",
    "2. Fit the model using the training data.\n",
    "3. Use the fit model to generate predictions.\n",
    "4. Evaluate our model's performance and, if necessary, return to step 2 and make changes.\n",
    "\n",
    "In this case, we've always had exactly one model. Today, however, we're going to talk about **ensemble methods**. Mentally, you should think about this as if we build multiple models and then aggregate their results in some way.\n",
    "\n",
    "### Why would we build an \"ensemble model?\"\n",
    "Consider $H$ the space of all possible hypotheses. Our goal is to estimate $f$, the true function. We can come up with different hypotheses $h_1$, $h_2$, and so on to get as close to $f$ as possible. \n",
    "- Think about $f$ as the true process that dictates Iowa liquor sales.\n",
    "- Think about $h_1$ as the model you built to predict $f$.\n",
    "\n",
    "![](./assets/images/ensemble-benefits.png)\n",
    "\n",
    "- The **statistical** benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate liquor sales; predictions from another model might underestimate liquor sales. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function $f$.\n",
    "- The **computational** benefit to ensemble methods: It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions and that all generalized linear models iterate toward a solution that isn't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART or an individual GLM to arrive at the true function $f$. However, starting our \"local searches\" at different points and aggregating our predictions might .\n",
    "- The **representational** benefit to ensemble methods: Even if we had all the data and all the computer power in the world, ot might be impossible for one model to exactly equal $f$. For example, a linear regression model can never model a relationship where a one-unit change in $X$ effects some *different* change in $Y$ based on the value of $X$. All models have some shortcomings. (See [the no free lunch theorems](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization).) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Let's get started actually making ensemble predictions. However, in order to do that, we'll need to introduce the idea of bootstrapping, or random sampling **with replacement.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bootstrap(data, stat = np.mean, size = 10):\n",
    "    stat_list = []\n",
    "    for i in range(size):\n",
    "        statistic = stat(np.random.choice(data, size = len(data), replace = False))\n",
    "        stat_list.append(statistic)\n",
    "    return stat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a88743ef2e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-97d05d16d909>\u001b[0m in \u001b[0;36mbootstrap\u001b[0;34m(data, stat, size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstat_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mstatistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mstat_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstat_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "bootstrap(lst, size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Decision Trees\n",
    "\n",
    "As we have seen, decision trees are very powerful machine learning models. However, decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). \n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-69838e874a74b9537c2540c65dbea725)\n",
    "\n",
    "Bagging (bootstrap aggregating) helps to mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n",
    "\n",
    "The process for creating bagged decision trees is as follows:\n",
    "1. From the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!).\n",
    "2. Build a decision tree on each bootstrapped sample.\n",
    "3. Make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n",
    "\n",
    "![www.cse.buffalo.edu/~jing/sdm10ensemble.htm](./assets/images/Ensemble.png)\n",
    "\n",
    "### What do you mean by \"aggregate prediction?\"\n",
    "As with all of our modeling techniques, we want to make sure that we can come up with one final prediction for our observation. (Building 1,000 trees and coming up with 1,000 predictions for one observation probably wouldn't be very helpful.)\n",
    "\n",
    "Suppose we want to predict whether or not a Reddit post is going to go viral, where `1` indicates viral and `0` indicates non-viral. We build 100 decision trees. Given a new Reddit post labeled `X_test`, we pass these features into all 100 decision trees.\n",
    "- 70 of the trees predict that the post in `X_test` will go viral.\n",
    "- 30 of the trees predict that the post in `X_test` will not go viral.\n",
    "\n",
    "<details><summary> What might you expect `.predict(X_test)` and `.predict_proba(X_test)` to do?\n",
    "</summary>\n",
    "```\n",
    "- .predict(X_test) should output a 1, predicting that the post will go viral.\n",
    "- .predict_proba(X_test) should output 0.7, indicating the probability of the post going viral is 70%.\n",
    "```\n",
    "</details>\n",
    "\n",
    "- **Discrete:** In ensemble methods, we will most commonly predict a discrete $Y$ by \"plurality vote,\" where the most common class is the predicted value for a given observation.\n",
    "- **Continuous:** In ensemble methods, we will most commonly predict a continuous $Y$ by averaging the predicted values into one final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "With bagged decision trees, we generate many different trees on pretty similar data. These trees are **strongly correlated** with one another. Because these trees are correlated with one another, they will have high variance. Looking at the variance of the average of two random variables $T_1$ and $T_2$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Var\\left(\\frac{T_1+T_2}{2}\\right) &=& \\frac{1}{4}\\left[Var(T_1) + Var(T_2) + 2Cov(T_1,T_2)\\right]\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "If $T_1$ and $T_2$ are highly correlated, then the variance will about as high as we'd see with individual decision trees. By \"decorrelating\" our trees from one another, we can drastically reduce the variance of our model.\n",
    "\n",
    "That's the difference between bagged decision trees and random forests! We're going to do the same thing as before, but we're going to decorrelate our trees. This will reduce our variance (at the expense of a small increase in bias) and thus should greatly improve the overall performance of the final model.\n",
    "\n",
    "### So how do we \"decorrelate\" our trees?\n",
    "Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n",
    "\n",
    "The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of the features at each split, we counter this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "For a problem with $p$ features, it is typical to use:\n",
    "\n",
    "- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n",
    "- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n",
    "\n",
    "While this is a guideline, Hastie and Tibshirani (authors of Introduction to Statistical Learning and Elements of Statistical Learning) have suggested this as a good rule in the absence of some rationale to do something different.\n",
    "\n",
    "Random forests, a step beyond bagged decision trees, are **very widely used** classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n",
    "- It is quite common for interviewers to ask how a random forest is constructed or how it is superior to a single decision tree.\n",
    "\n",
    "--- \n",
    "\n",
    "## Extremely Randomized Trees (ExtraTrees)\n",
    "Adding one more step of randomization (and thus decorrelation) yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging (sampling of observations) and the random subspace method (sampling of features), but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity) for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range.\n",
    "\n",
    "This further reduces the variance, but causes an increase in bias. If you're considering using ExtraTrees, you might consider this to be a hyperparameter you can tune. Build an ExtraTrees model, a Random Forest model, and a Bagged Decision Treesmodel, then compare their performance!\n",
    "\n",
    "That's exactly what we'll do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Random Forest and ExtraTrees in Scikit Learn\n",
    "\n",
    "Scikit Learn implements both random forest and extra trees methods as part of the `ensemble` module.\n",
    "\n",
    "Have a look at the [documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest).\n",
    "\n",
    "**Check:** What parameters did you notice? Any questions on those?\n",
    "\n",
    "Let's load the [car dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/).\n",
    "\n",
    "We would like to compare the performance of the following 4 algorithms:\n",
    "\n",
    "- Decision Trees\n",
    "- Bagging + Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "Note that in order for our results to be consistent, we have to expose the models to exactly the same CrossValidation scheme. Let's start by initializing that. Then, we'll initialize the models.\n",
    "\n",
    "*You can also create a function to speed up your work...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(df['acceptability']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = BaggingClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = RandomForestClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = ExtraTreesClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Extra Trees\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the effect of balancing our classes on the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree with Balanced Classes\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = BaggingClassifier(class_weight = 'balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging with Balanced Classes\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No balancing class argument for bagged classifier. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = RandomForestClassifier(class_weight = 'balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest with Balanced Classes\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = ExtraTreesClassifier(class_weight = 'balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Extra Trees with Balanced Classes\", s.mean().round(3), s.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can improve the performance of a single model by generating multiple models and aggregating their predictions. \n",
    "- If our $Y$ is continuous, we average each prediction. \n",
    "- If our $Y$ is discrete, we use a \"plurality vote\" to decide the predicted value.\n",
    "\n",
    "The three types of ensemble models we discussed today:\n",
    "1. **Bagged Decision Trees** are where we take the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!), build a decision tree on each sample, then make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n",
    "2. **Random Forests** are where we bag decision trees, but when it comes to building each individual decision tree, we only consider a random subset of features at each split.\n",
    "3. **ExtraTrees** are where we build random forests, but when it comes to building each individual decision tree, we also select a random split of each feature at each node.\n",
    "\n",
    "Some of these methods will perform better in some cases, some better in other cases. For example, decision trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand, ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain. (This gets back to our discussion about prediction versus inference - only you and your stakeholders will recognize what balance to strike between these two!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Ensemble Methods Paper](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
