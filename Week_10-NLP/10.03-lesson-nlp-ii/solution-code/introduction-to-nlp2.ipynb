{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Intro to nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Extract features from unstructured text with scikit-learn\n",
    "- Describe how count vectorization and TF-IDF work.\n",
    "- Define stop words and remove them with scikit-learn.\n",
    "- Identify shortcomings with the aforementioned methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to Text Feature Extraction\n",
    "\n",
    "---\n",
    "\n",
    "The models we’ve been using so far accept a two-dimensional matrix of real numbers as input `X` and a target vector of classes or numbers as `y`. What if our starting point data are not given in the form of a table of numbers but rather are unstructured? This is the case when we work with text documents.\n",
    "\n",
    "> We need a way to go from unstructured data to our numeric `X` matrix in order to use the same models. This is called _feature extraction_, and this lesson is dedicated to it.\n",
    "\n",
    "The applications of using text data in statistical modeling are practically infinite. Some examples include:\n",
    "\n",
    "- Sentiment analysis of Yelp reviews.\n",
    "- Identifying topics of new articles.\n",
    "- Classification of political authors.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "# Y ~ \"YOLO 4life ^^ BBQ@@ OMG LOL!\"\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://snag.gy/FoaBeK.jpg\" style=\"width: 400px; float: left; margin-right: 20px;\">\n",
    "\n",
    "<img src=\"https://snag.gy/Qz0mav.jpg\" style=\"width: 150px; float: left; margin-right: 50px;\">\n",
    "\n",
    "<img src=\"https://snag.gy/6Lu9aC.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='common'></a>\n",
    "## Common NLP Problems\n",
    "\n",
    "---\n",
    "\n",
    "The table below details some of the most common problems and tasks in the vast field of natural language processing (NLP).\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| **Sentiment Analysis** | Determining if what is written is positive or negative. | \n",
    "| **Named Entity Recognition** | Classifying names of people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. |\n",
    "| **Summarization** | Boiling down large bodies of text to paraphrased versions. |\n",
    "| **Topic Modeling** | Pinpointing the topics a body of text belongs to (e.g., auto-tagging news articles). |\n",
    "| **Question Answering** | Determining the answer to a human-language question. |\n",
    "| **Word Disambiguation** | Many words have more than one meaning; we have to select the meaning that makes the most sense in context. For this problem, we’re typically given a list of words and associated word senses (e.g., from a dictionary or from an online resource such as WordNet). |\n",
    "| **Machine Dialog Systems** | Building response systems that react contextually to human input (i.e., Me: \"Siri, cook me some bacon.\" Siri: \"How do you like your bacon cooked?\"). | \n",
    "\n",
    "\n",
    "See also:\n",
    "\n",
    "- [News headline analysis](http://nbviewer.jupyter.org/github/AYLIEN/headline_analysis/blob/06f1223012d285412a650c201a19a1c95859dca1/main-chunks.ipynb?utm_content=buffer5d40c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer).\n",
    "- [Sentiment and robot classification in movies](http://nbviewer.jupyter.org/github/cojette/ClusteringRobotsinMovie/blob/master/Classification%20of%20Robots%20in%20Movies.ipynb).\n",
    "- [Text summarization with Gensim](http://nbviewer.jupyter.org/github/piskvorky/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb).\n",
    "- [Sentiment analysis introduction](http://nbviewer.jupyter.org/github/sgsinclair/alta/blob/master/ipynb/SentimentAnalysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='models'></a>\n",
    "## Some Common NLP Models and Terms\n",
    "\n",
    "---\n",
    "\n",
    "- Latent semantic indexing (LSI)\n",
    "- Latent dirichlet allocation (LDA)\n",
    "- Hierarchical dirichlet process (HDP)\n",
    "- Word2Vec\n",
    "- LogisticRegression\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- CountVectorizer\n",
    "- Term frequency-inverse document frequency (TF-IDF)\n",
    "- Document term matrix (DTM)\n",
    "\n",
    "> **Note:** This is not an exhaustive list, nor will we be covering all of these models in class. NLP is a very deep and broad area of data science that could warrant its own Immersive course entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='simple'></a>\n",
    "## A Simple Example\n",
    "---\n",
    "\n",
    "Suppose we're building a spam classifier. The inputs are emails and the output is a binary label.\n",
    "\n",
    "Here's an example of an input email from each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer, chairman of the board of directors of PJSC \"LUKOIL.\" I am 86-years old and I was diagnosed with cancer two years ago. I will be going in for an operation later this week. I decided to will/donate the sum of 8,750,000.00 Euros (eight million seven hundred and fifty thousand euros only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of data scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews, and we would like to invite you for an onsite interview with our senior data scientist, Mr. John Smith. You will find attached to this message further information on date, time, and location of the interview. Please let me know if I can be of any further assistance. Best regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer, chairman of the board of directors of PJSC \"LUKOIL.\" I am 86-years old and I was diagnosed with cancer two years ago. I will be going in for an operation later this week. I decided to will/donate the sum of 8,750,000.00 Euros (eight million seven hundred and fifty thousand euros only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of data scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews, and we would like to invite you for an onsite interview with our senior data scientist, Mr. John Smith. You will find attached to this message further information on date, time, and location of the interview. Please let me know if I can be of any further assistance. Best regards.\n",
    "\"\"\"\n",
    "\n",
    "print(spam)\n",
    "print()\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "Virtually all NLP uses this base terminology:\n",
    "\n",
    "- a collection of text is a **document**\n",
    "- a collection of documents is a **corups** (plural corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [spam, ham]  # two docs in our corups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can You Think of a Simple Heuristic Rule to Catch Emails Like This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> _We could check for the presence of the words \"donate,\" \"will,\" \"sum,\" \"cancer,\" \"LinkedIn,\" and those that are similar._\n",
    "\n",
    "By defining a simple rule that parses the text for the presence of keywords, we’re performing one of the simplest text feature extraction methods: _binary word counting_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bow'></a>\n",
    "## Bag of Words/Word Counting\n",
    "---\n",
    "\n",
    "The bag-of-words model is a simplified representation of the raw data. In this model, text (such as a sentence or document) is represented as the bag (multiset) of its words.\n",
    "\n",
    "Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"countvectorizer\"></a>\n",
    "## Demo: Scikit-Learn `CountVectorizer`\n",
    "---\n",
    "\n",
    "Scikit-learn offers a `CountVectorizer` class with many configurable options:\n",
    "\n",
    "**Note**: There are several parameters to tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "#cvec = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x112 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 130 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.fit(corpus)\n",
    "new_corpus = cvec.transform(corpus)\n",
    "new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         2, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1,\n",
       "         0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 4, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1,\n",
       "         2, 2, 0, 0, 2, 2, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "         0, 0, 4, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2,\n",
       "         1, 0, 2, 0, 1, 0, 1, 0, 1, 3, 1, 0, 0, 1, 5, 0, 0, 0, 2, 0, 0,\n",
       "         1, 1, 1, 1, 0, 4, 1]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>in</th>\n",
       "      <th>etc</th>\n",
       "      <th>is</th>\n",
       "      <th>have</th>\n",
       "      <th>contact</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      of  and  your  in  etc  is  have  contact  the  this\n",
       "spam   4    3     2   2    2   2     2        2    2     2\n",
       "ham    4    2     1   1    0   0     0        0    3     1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(new_corpus.todense(),\n",
    "                   columns=cvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "df.T.sort_values('spam', ascending=False).head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spend a couple of minutes scanning the documentation to figure out what those parameters do.\n",
    "\n",
    "In groups, share a few takeaways from the documentation.  In particular, look at what the following do:\n",
    "\n",
    "- `encoding`\n",
    "- `analyzer`\n",
    "- `lowercase`\n",
    "\n",
    "What arguments and capabilities stand out to you? \n",
    "\n",
    "We will soon look at stop words, documents and n-grams which are mentioned in the docs.\n",
    "\n",
    "[Count vectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "## Stop Words\n",
    "\n",
    "---\n",
    "\n",
    "Some words are commonly used and provide no legitimate information about the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'only', 'etc', 'anyone', 'serious', 'whereafter', 'after', 'please', 'both', 'against', 'moreover', 'too', 'amoungst', 'meanwhile', 'i', 'three', 'until', 'first', 'neither', 'what', 'they', 'per', 'get', 'yourselves', 'system', 'us', 'was', 'where', 'very', 'became', 'top', 'although', 'cry', 'beyond', 'about', 'any', 'along', 'between', 'five', 'fire', 'none', 'side', 'show', 'anything', 'am', 'one', 'third', 'cant', 'wherever', 'somewhere', 'much', 'sometimes', 'with', 'how', 'my', 'four', 'towards', 'whenever', 'yourself', 're', 'whereby', 'due', 'herself', 'thin', 'has', 'itself', 'next', 'hereupon', 'some', 'there', 'during', 'before', 'been', 'except', 'it', 'upon', 'their', 'had', 'no', 'while', 'me', 'these', 'you', 'name', 'often', 'from', 'last', 'even', 'un', 'another', 'every', 'mostly', 'over', 'put', 'several', 'someone', 'at', 'thus', 'must', 'all', 'found', 'other', 'seemed', 'beforehand', 'hasnt', 'above', 'couldnt', 'though', 'or', 'eleven', 'hence', 'everywhere', 'its', 'so', 'something', 'toward', 'without', 'thereupon', 'than', 'most', 'once', 'besides', 'we', 'an', 'fifteen', 'twenty', 'however', 'be', 'would', 'via', 'she', 'amongst', 'describe', 'eg', 'thence', 'onto', 'each', 'others', 'either', 'up', 'again', 'whole', 'elsewhere', 'everything', 'below', 'almost', 'empty', 'cannot', 'he', 'bill', 'many', 'perhaps', 'seeming', 'throughout', 'for', 'to', 'which', 'in', 'more', 'a', 'enough', 'somehow', 'because', 'beside', 'former', 'front', 'nor', 'through', 'inc', 'around', 'nevertheless', 'detail', 'seems', 'here', 'alone', 'noone', 'whereupon', 'twelve', 'off', 'ie', 'formerly', 'hereafter', 'himself', 'becoming', 'full', 'should', 'co', 'latter', 'the', 'were', 'yet', 'whereas', 'few', 'are', 'by', 'everyone', 'latterly', 'his', 'amount', 'is', 'nowhere', 'thick', 'within', 'on', 'take', 'never', 'whether', 'go', 'keep', 'and', 'nine', 'hers', 'such', 'see', 'together', 'if', 'indeed', 'could', 'six', 'themselves', 'why', 'among', 'afterwards', 'behind', 'mine', 'otherwise', 'ten', 'hereby', 'own', 'that', 'thereafter', 'becomes', 'find', 'least', 'seem', 'then', 'down', 'move', 'being', 'interest', 'nothing', 'rather', 'eight', 'when', 'call', 'become', 'forty', 'bottom', 'made', 'yours', 'less', 'but', 'of', 'part', 'same', 'give', 'mill', 'sincere', 'whither', 'done', 'whoever', 'those', 'ours', 'since', 'can', 'across', 'herein', 'hundred', 'well', 'wherein', 'will', 'ltd', 'into', 'do', 'our', 'else', 'sometime', 'whom', 'sixty', 'anyway', 'them', 'anyhow', 'who', 'whatever', 'your', 'further', 'this', 'nobody', 'de', 'out', 'fifty', 'anywhere', 'already', 'may', 'now', 'two', 'might', 'namely', 'ever', 'myself', 'as', 'also', 'still', 'whence', 'thru', 'back', 'have', 'therein', 'therefore', 'always', 'him', 'not', 'under', 'ourselves', 'whose', 'thereby', 'fill', 'her', 'con'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    " \n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hash'></a>\n",
    "## What is a Hash Function?\n",
    "\n",
    "---\n",
    "![](https://i.ytimg.com/vi/bs7Wq0Z1uYk/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing\n",
    "\n",
    "A hash value is a number generated from a string of text. It's also referred to simply as \"hash\" or \"message digest.\"\n",
    "\n",
    "The hash is substantially smaller than the text itself and is generated by a formula in such a way that it's extremely unlikely some other text will produce the same hash value.\n",
    "\n",
    "Think of the hash as a code that represents the original text in a more condensed format.\n",
    "\n",
    "![](images/hash_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"hashingvectorizer\"></a>\n",
    "## Scikit-Learn's `HashingVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "As we’ve seen, we can set the `CountVectorizer` dictionary to a fixed size, only keeping words of certain frequencies. However, **we still have to compute a dictionary and hold it in memory.** This could be a problem when...\n",
    "\n",
    "- we have a large corpus or \n",
    "- when we stream applications where we don't know which words we'll encounter in the future.\n",
    "\n",
    "Both problems can be solved using the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences calculated with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each word is mapped to a feature with the use of a [hash function](https://en.wikipedia.org/wiki/Hash_function), which converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurrence without retaining a dictionary in memory.\n",
    "\n",
    "**Huh?**\n",
    "\n",
    "$$\n",
    "\\text{word} \\rightarrow \\text{hash} \\rightarrow \\text{column index}\n",
    "$$\n",
    "\n",
    "The main drawback of this trick is that it's *not possible to compute the inverse transform* and we lose information on which words correspond with the important features. The hash function employed is the signed 32-bit version of Murmurhash3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using the code above as an example, let's repeat the vectorization using a `HashingVectorizer`.\n",
    "\n",
    "Look up how to complete this step and then try it out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>479532</th>\n",
       "      <th>170062</th>\n",
       "      <th>994433</th>\n",
       "      <th>144749</th>\n",
       "      <th>832412</th>\n",
       "      <th>675997</th>\n",
       "      <th>1005907</th>\n",
       "      <th>174171</th>\n",
       "      <th>828689</th>\n",
       "      <th>134503</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.336861</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.16843</td>\n",
       "      <td>0.16843</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>0.084215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.334497</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.334497</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>0.418121</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       479532    170062   994433   144749    832412    675997    1005907  \\\n",
       "spam  0.336861  0.168430  0.16843  0.16843  0.168430  0.168430  0.168430   \n",
       "ham   0.334497  0.083624  0.00000  0.00000  0.334497  0.083624  0.083624   \n",
       "\n",
       "       174171    828689    134503   \n",
       "spam  0.168430  0.168430  0.084215  \n",
       "ham   0.418121  0.083624  0.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hvec = HashingVectorizer()\n",
    "hvec.fit(corpus)\n",
    "\n",
    "df  = pd.DataFrame(hvec.transform(corpus).todense(), index=['spam', 'ham'])  \n",
    "\n",
    "df.T.sort_values('spam', ascending=False).head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What new parameters does this vectorizer offer?\n",
    "\n",
    "Go back to the documentation and compare it to `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer:\n",
    "- n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"downsides-bow\"></a>\n",
    "## Downsides to Bag of Words\n",
    "\n",
    "---\n",
    "\n",
    "Bag-of-word approaches like the one outlined above completely ignore the structure of a sentence and merely assess the presence of specific words or word combinations.\n",
    "\n",
    "But the same word can have multiple meanings in different contexts. Consider, for example, the following two sentences:\n",
    "\n",
    "- There's wood floating in the **sea**.\n",
    "- Mike's in a **sea** of trouble with the move.\n",
    "\n",
    "How do we teach a computer to disambiguate? We'll cover some techniques that may help with this a little later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"tfidf\"></a>\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "---\n",
    "\n",
    "A TF-IDF score tells us which words are most discriminating between documents. Words that occur often in one document but don't occur in many documents contain a great deal of discriminating power.\n",
    "\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection (corpus).\n",
    "- The importance increases in proportion to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Variations of the TF-IDF weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides — that is, whether the term is common or rare across all documents. It's the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term and then taking the logarithm of that quotient.\n",
    "\n",
    "**Let's see how it's calculated:**\n",
    "\n",
    "Term frequency (`tf`) is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{term}$ is the number of times a term/word $t$ appears in document $d$\n",
    "- $N_\\text{terms in Document}$ is the number of terms/words in document $d$\n",
    "\n",
    "Inverse document frequency (`idf`) is defined as the frequency of documents that contain that term over the whole corpus:\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{Documents}$ is the number of documents in the corpus $D$\n",
    "- $N_\\text{Documents that contain term}$ is the number of documents in $D$ that contain term/word $t$\n",
    "\n",
    "TF-IDF is then calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **You might ask: But what is `log` used for?**<br>\n",
    "> Good question! This is a sublinear transformation that helps separate our extremes between rare and common values.\n",
    "\n",
    "> \"...any linear function, ${\\displaystyle g}$, for sufficiently large input ${\\displaystyle f}$, grows slower than ${\\displaystyle g}$\" — Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='tfidf-vec'></a>\n",
    "## Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use TF-IDF?\n",
    "- Common words are penalized.\n",
    "- Rare words have more influence.\n",
    "\n",
    "Scikit-learn provides a TF-IDF vectorizer that works similarly to the other vectorizers we've covered. Notice that we can also eliminate stop words to improve our analysis.\n",
    "\n",
    "As you did above, import and initialize the `TfidfVectorizer`, then fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>contact</th>\n",
       "      <th>personality</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>old</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         years     euros   contact  personality  linkedin    lukoil     major  \\\n",
       "spam  0.290133  0.290133  0.290133     0.145067  0.145067  0.145067  0.145067   \n",
       "ham   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       million       old  operation  \n",
       "spam  0.145067  0.145067   0.145067  \n",
       "ham   0.000000  0.000000   0.000000  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(tvec.transform(corpus).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "\n",
    "df.transpose().sort_values('spam', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scientist</th>\n",
       "      <th>regards</th>\n",
       "      <th>data</th>\n",
       "      <th>interview</th>\n",
       "      <th>round</th>\n",
       "      <th>hooli</th>\n",
       "      <th>inform</th>\n",
       "      <th>interviews</th>\n",
       "      <th>invite</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scientist  regards     data  interview     round     hooli    inform  \\\n",
       "spam    0.00000  0.00000  0.00000    0.00000  0.000000  0.000000  0.000000   \n",
       "ham     0.31039  0.31039  0.31039    0.31039  0.155195  0.155195  0.155195   \n",
       "\n",
       "      interviews    invite      like  \n",
       "spam    0.000000  0.000000  0.000000  \n",
       "ham     0.155195  0.155195  0.155195  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transpose().sort_values('ham', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Real\" Example\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this stuff out on some SMS text data.  Can you predict real vs. promotional texts just based on what is written?  Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>Is that seriously how you spell his name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>So ü pay first lar... Then when is da stock co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aft i finish my lunch then i go str down lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just forced myself to eat a slice. I'm really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm back &amp;amp; we're packing the car now, I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ahhh. Work. I vaguely remember that! What does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wait that's still not all that clear, were you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah he got in at 2 and was v apologetic. n ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ham</td>\n",
       "      <td>K tell me anything about you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ham</td>\n",
       "      <td>For fear of fainting with the of all that hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spam</td>\n",
       "      <td>Thanks for your subscription to Ringtone UK yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup... Ok i go home look at the timings then i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oops, I'll let you know when my roommate's done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ham</td>\n",
       "      <td>I see the letter B on my car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ham</td>\n",
       "      <td>Anything lor... U decide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello! How's you and how did saturday go? I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pls go ahead with watts. I just wanted to be s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did I forget to tell you ? I want you , I need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>spam</td>\n",
       "      <td>07732584351 - Rodger Burns - MSG = We tried to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ham</td>\n",
       "      <td>WHO ARE YOU SEEING?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ham</td>\n",
       "      <td>Great! I hope you like your man well endowed. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ham</td>\n",
       "      <td>Didn't you get hep b immunisation in nigeria.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fair enough, anything going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah hopefully, if tyler can't do it I could m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ham</td>\n",
       "      <td>U don't know how stubborn I am. I didn't even ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    ham  Even my brother is not like to speak with me. ...\n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8   spam  WINNER!! As a valued network customer you have...\n",
       "9   spam  Had your mobile 11 months or more? U R entitle...\n",
       "10   ham  I'm gonna be home soon and i don't want to tal...\n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12  spam  URGENT! You have won a 1 week FREE membership ...\n",
       "13   ham  I've been searching for the right words to tha...\n",
       "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15  spam  XXXMobileMovieClub: To use your credit, click ...\n",
       "16   ham                         Oh k...i'm watching here:)\n",
       "17   ham  Eh u remember how 2 spell his name... Yes i di...\n",
       "18   ham  Fine if thats the way u feel. Thats the way ...\n",
       "19  spam  England v Macedonia - dont miss the goals/team...\n",
       "20   ham          Is that seriously how you spell his name?\n",
       "21   ham    I‘m going to try for 2 months ha ha only joking\n",
       "22   ham  So ü pay first lar... Then when is da stock co...\n",
       "23   ham  Aft i finish my lunch then i go str down lor. ...\n",
       "24   ham  Ffffffffff. Alright no way I can meet up with ...\n",
       "25   ham  Just forced myself to eat a slice. I'm really ...\n",
       "26   ham                     Lol your always so convincing.\n",
       "27   ham  Did you catch the bus ? Are you frying an egg ...\n",
       "28   ham  I'm back &amp; we're packing the car now, I'll...\n",
       "29   ham  Ahhh. Work. I vaguely remember that! What does...\n",
       "30   ham  Wait that's still not all that clear, were you...\n",
       "31   ham  Yeah he got in at 2 and was v apologetic. n ha...\n",
       "32   ham                      K tell me anything about you.\n",
       "33   ham  For fear of fainting with the of all that hous...\n",
       "34  spam  Thanks for your subscription to Ringtone UK yo...\n",
       "35   ham  Yup... Ok i go home look at the timings then i...\n",
       "36   ham    Oops, I'll let you know when my roommate's done\n",
       "37   ham                       I see the letter B on my car\n",
       "38   ham                        Anything lor... U decide...\n",
       "39   ham  Hello! How's you and how did saturday go? I wa...\n",
       "40   ham  Pls go ahead with watts. I just wanted to be s...\n",
       "41   ham  Did I forget to tell you ? I want you , I need...\n",
       "42  spam  07732584351 - Rodger Burns - MSG = We tried to...\n",
       "43   ham                                WHO ARE YOU SEEING?\n",
       "44   ham  Great! I hope you like your man well endowed. ...\n",
       "45   ham                   No calls..messages..missed calls\n",
       "46   ham      Didn't you get hep b immunisation in nigeria.\n",
       "47   ham                    Fair enough, anything going on?\n",
       "48   ham  Yeah hopefully, if tyler can't do it I could m...\n",
       "49   ham  U don't know how stubborn I am. I didn't even ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./sms.csv', index_col=0)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865985\n",
       "spam    0.134015\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['class'].map({'ham':0, 'spam':1})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_counts = cvec.fit_transform(X_train)\n",
    "X_test_counts = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820652173913044"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_counts, y_train)\n",
    "log_reg.score(X_test_counts, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "In this lesson, we covered an overview of natural language processing and learned about two powerful toolkits:\n",
    "\n",
    "- Scikit-Learn's Feature Extraction Text\n",
    "- Natural Language Toolkit\n",
    "\n",
    "**Check:** What are some real-world applications of these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spam detection.\n",
    "- Preprocessing for larger NLP problems.\n",
    "- Job market analysis.\n",
    "- Determining if someone is dateable or not; identifying \"I\" in relation to the signifier.\n",
    "- Crude topic analysis.\n",
    "- Building a keyword extraction heuristic and piping it into a marketing analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- Check out this [Yelp blog post](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) on how it completed a classification task (with more than 1,000 response variables) using restaurant review text.\n",
    "- Always check documentation: \n",
    "    - [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "    - [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). \n",
    "    - [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "- A list of all stop words is available [here](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-05/5.04-nlp/stop-words.txt).\n",
    "- Wikipedia's [feature hashing](https://github.com/generalassembly-studio/DSI-course-materials/tree/master/curriculum/04-lessons/week-06/4.1-lesson) and [hash functions](https://en.wikipedia.org/wiki/Hash_function) entries are a great place to turn for more information on hashing.\n",
    "- Check out Charlie Greenbacker's [introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf), which he delivered at the [DC-NLP Meetup](http://www.meetup.com/DC-NLP/).\n",
    "- Wikipedia also has a [walk through](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) of TF-IDF.\n",
    "- We played with Google's [ngram tool](https://books.google.com/ngrams/graph?content=data+science&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cdata%20science%3B%2Cc0).\n",
    "- A hilarious data scientist has gone rogue and used NLP and eigenfaces (eigenvalues for face recognition) [for Tinder](http://dataconomy.com/hacking-tinder-with-facial-recognition-nlp/).\n",
    "- We referenced KPCB's 2016 internet trends. If you're into startups, check out [this insightful deck](http://www.kpcb.com/internet-trends).\n",
    "- [Count vectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "- [Choosing a stemmer](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html).\n",
    "- [Feature hashing](https://en.wikipedia.org/wiki/Feature_hashing).\n",
    "- [Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "- [TF-IDF vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rapstats'></a>\n",
    "## And just for fun... An Example NLP Project: rapstats.io\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"http://rapstats.io\"><img src=\"https://snag.gy/8GSVqf.jpg\"></a>\n",
    "\n",
    "<img src=\"https://snag.gy/8eJNFv.jpg\" style=\"width: 300px; float: left;\">\n",
    "<img src=\"https://snag.gy/2Hz0o7.jpg\" style=\"width: 300px;\">\n",
    "\n",
    "**See also:**\n",
    "\n",
    "- [The Largest Vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html).\n",
    "- [Rap Genius: Rap Stats](http://genius.com/rapstats).\n",
    "- [Rap lyric generator, Hieu Nguyen and Brian Sa](http://nlp.stanford.edu/courses/cs224n/2009/fp/5.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
