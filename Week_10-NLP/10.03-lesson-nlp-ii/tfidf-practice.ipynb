{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Practice\n",
    "\n",
    "Today we will be taking a closer look at TFIDF as a descriptive measure of rare terms, and practicing some basic plots with TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and setup code\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn's version of tfidf\n",
    "\n",
    "Deep in the swamp of sklearn's documentation, lies some useful information about their exact implementation.\n",
    "\n",
    "```\n",
    "    The formula that is used to compute the tf-idf of term t is\n",
    "    tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as\n",
    "    idf(d, t) = log [ n / df(d, t) ] + 1 (if ``smooth_idf=False``),\n",
    "    where n is the total number of documents and df(d, t) is the\n",
    "    document frequency; the document frequency is the number of documents d\n",
    "    that contain term t. The effect of adding \"1\" to the idf in the equation\n",
    "    above is that terms with zero idf, i.e., terms  that occur in all documents\n",
    "    in a training set, will not be entirely ignored.\n",
    "    (Note that the idf formula above differs from the standard\n",
    "    textbook notation that defines the idf as\n",
    "    idf(d, t) = log [ n / (df(d, t) + 1) ]).\n",
    "    \n",
    "    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
    "    numerator and denominator of the idf as if an extra document was seen\n",
    "    containing every term in the collection exactly once, which prevents\n",
    "    zero divisions: idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.\n",
    "    Furthermore, the formulas used to compute tf and idf depend\n",
    "    on parameter settings that correspond to the SMART notation used in IR\n",
    "    as follows:\n",
    "    \n",
    "    Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
    "    ``sublinear_tf=True``.\n",
    "    Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
    "    Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
    "    when ``norm=None``.\n",
    "```\n",
    "> https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/feature_extraction/text.py#L941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf from scratch\n",
    "\n",
    "Our version will differ slightly from sklearn's version but the effect of rare words will be the same.  Mainly, sklearn's version will normalize and handle additional transformation features.  Our example will look at a specific term and calculate it's tf-idf for a given document, relative to all documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\"I am a cat\", \"I am a bat\", \"I am a rat\"] # which animal are you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to \"tokenize\" each list of sentences to a list of words.\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "# Calculates the frequency of a term in a document\n",
    "def term_frequency(term, document):\n",
    "    return len([word for word in document if word == term]) / float(len(document))\n",
    "\n",
    "# log(total documents / documents containing a \n",
    "def inverse_document_frequency(term, corpus):\n",
    "    \n",
    "    total_documents = len(corpus)\n",
    "    documents_with_term = len([doc for doc in corpus if term in doc])\n",
    "    \n",
    "    print \"The term '%s' appears in %d document(s) of %d total document(s)\" % (term, documents_with_term, total_documents)\n",
    "    \n",
    "    print \"non logged IDF: \", float(total_documents) / float(documents_with_term)\n",
    "    \n",
    "    return np.log(float(total_documents) / documents_with_term) + 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try our code out\n",
    "- Common terms\n",
    "- Rare terms\n",
    "- Different documents / terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a cat', 'I am a bat', 'I am a rat']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The term 'i' appears in 3 document(s) of 3 total document(s)\n",
      "non logged IDF:  1.0\n",
      "\n",
      "Term Frequency:  0.25\n",
      "Inverse Document Frequency:  1.0\n",
      "TF*IDF:  0.25\n"
     ]
    }
   ],
   "source": [
    "# Each sentence becomes a list of words\n",
    "tokenized_documents = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "# # The term we are looking at\n",
    "term = \"i\"\n",
    "\n",
    "# # The term in the SPECIFIC document we care about\n",
    "tf = term_frequency(term, tokenized_documents[0])\n",
    "\n",
    "# # Basic TFIDF\n",
    "idf = inverse_document_frequency(term, tokenized_documents)\n",
    "idf\n",
    "\n",
    "print\n",
    "print \"Term Frequency: \", tf\n",
    "print \"Inverse Document Frequency: \", idf\n",
    "print \"TF*IDF: \", tf * 1.0 * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic example, revisited.\n",
    "\n",
    "To understand TFIDF is to use it at a basic level, again.  For our mini-practice today, we will revisit the cat, rat, and the bat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\"I am a cat.\", \"I am a bat.\", \"I am a rat.\"] # which animal are you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorize your corpus using TfidfVectorizer and set to a variable called \"X\"\n",
    "1. Initialize TfidfVectorizer to a new variable.\n",
    "1. Set X to the return of fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>am</th>\n",
       "      <th>bat</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>rat</th>\n",
       "      <th>that</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.539725</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>sanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.474961</td>\n",
       "      <td>0.280520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280520</td>\n",
       "      <td>0.361220</td>\n",
       "      <td>0.361220</td>\n",
       "      <td>0.474961</td>\n",
       "      <td>0.361220</td>\n",
       "      <td>sanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       also        am       bat       cat        is       not       rat  \\\n",
       "0  0.000000  0.447214  0.000000  0.894427  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.318770  0.539725  0.318770  0.410475  0.410475  0.000000   \n",
       "2  0.474961  0.280520  0.000000  0.280520  0.361220  0.361220  0.474961   \n",
       "\n",
       "       that   target  \n",
       "0  0.000000    trump  \n",
       "1  0.410475  sanders  \n",
       "2  0.361220  sanders  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer to a new variable.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "\n",
    "# Set X to the return of fit_transform.\n",
    "corpus = [\"I am a cat cat.\", \"I am a bat that is not a cat.\", \"I am a rat that is also not a cat.\"] # which animal are you?\n",
    "y = [\"trump\", \"sanders\", \"sanders\"]\n",
    "\n",
    "X = tvec.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=tvec.get_feature_names())\n",
    "df['target'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "model = logreg.fit(X, df['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.49994553,  0.50005447],\n",
       "        [ 0.640828  ,  0.359172  ],\n",
       "        [ 0.64624573,  0.35375427]]),\n",
       " array(['trump', 'sanders', 'sanders'], dtype=object))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X), model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at your vectorized corpus, X, as an array or a dense matrix.\n",
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>bacon</th>\n",
       "      <th>better</th>\n",
       "      <th>canada</th>\n",
       "      <th>cheese</th>\n",
       "      <th>damn</th>\n",
       "      <th>eat</th>\n",
       "      <th>from</th>\n",
       "      <th>get</th>\n",
       "      <th>gonna</th>\n",
       "      <th>...</th>\n",
       "      <th>of</th>\n",
       "      <th>off</th>\n",
       "      <th>on</th>\n",
       "      <th>raise</th>\n",
       "      <th>store</th>\n",
       "      <th>taxes</th>\n",
       "      <th>that</th>\n",
       "      <th>there</th>\n",
       "      <th>those</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@dyerrington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>@trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@dyerrington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   always  bacon  better  canada  cheese  damn  eat  from  get  gonna  \\\n",
       "0       0      0       0       0       1     0    0     0    0      0   \n",
       "1       0      0       1       0       0     1    0     0    1      0   \n",
       "2       0      0       0       1       0     0    0     0    0      1   \n",
       "3       1      1       0       0       0     0    1     1    0      0   \n",
       "\n",
       "       ...       of  off  on  raise  store  taxes  that  there  those  \\\n",
       "0      ...        1    0   0      0      1      0     1      1      0   \n",
       "1      ...        0    1   0      0      0      0     0      0      1   \n",
       "2      ...        0    0   1      1      0      1     0      0      2   \n",
       "3      ...        0    0   0      0      0      0     0      0      0   \n",
       "\n",
       "         target  \n",
       "0        @trump  \n",
       "1  @dyerrington  \n",
       "2        @trump  \n",
       "3  @dyerrington  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = [\n",
    "    {\"text\": \"There is a lot of cheese in that store.\", \"label\": \"@trump\"},\n",
    "    {\"text\": \"Those damn kids better get off my lawn\", \"label\": \"@dyerrington\"},\n",
    "    {\"text\": \"Gonna raise those taxes on those mofos in Canada\", \"label\": \"@trump\"},\n",
    "    {\"text\": \"Always eat bacon, from Ireland.\", \"label\": \"@dyerrington\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(df['text'])\n",
    "\n",
    "text_df = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "text_df['target'] = df['label']\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intialize a new dataframe with an array from X.\n",
    "Also, set the columns to the TF-IDF vectorizer objects `.get_feature_names()` reference.  Without it, you won't be able to reference which word features correspond to which matrix column.\n",
    "\n",
    "Each row will coorespond to each document from the original corpus object.  Verify that each row matches the original dataset with the word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your dataframe here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate your data with mean, median, min, max.  Plot each of your results with a \"bar\" or \"barh\" figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Refactor your existing code into a function.\n",
    "- Your method should accept a corpus object, a vectorizer type (tfidf or countvectorizer), and aggregate function parameter (string or function -- your choice!).\n",
    "- Your method should output a figure\n",
    "\n",
    "An example use case of your code would be:\n",
    "> ```python\n",
    ">  corpus = [\"I am a rat\", \"I am a cat\", \"I am a bat\"]\n",
    ">\n",
    ">  # TFIDF plot with max aggregation\n",
    ">  vectorize_and_plot(corpus, vectorizer=\"tfidf\", agg_func=\"max\")\n",
    ">  [your plot here] \n",
    ">\n",
    ">  # COUNT plot with max aggreagation\n",
    ">  vectorize_and_plot(corpus, vectorizer=\"count\", agg_func=\"max\")\n",
    ">  [your plot here] \n",
    ">  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use your function to compare CountVectorizer vs TfidfVectorizer \n",
    "- Use the original corpus object\n",
    "- THEN try using the new corpus object below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Original corpus\n",
    "# corpus = [\"I am a cat.\", \"I am a bat.\", \"I am a rat.\"] # which animal are you?\n",
    "\n",
    "## New corpus\n",
    "# corpus = [\"I am a cat.\", \"I am a bat.\", \"I am a rat.\", \"the cat is not a rat\", \"There is not a cat that sat\"] # which animal are you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check out this awesome pipeline.\n",
    "- Fit the corpus object\n",
    "- Try to run a basic prediction\n",
    "\n",
    "_This is not a real-world probem but hopefully you get a sense of the basics of looking at text data, and it's application in sklearn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73318551367331852"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# setup our data\n",
    "\n",
    "\"\"\" We use this list to filter which categories we want from our sample newsgroups dataset \"\"\"\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "training_data = fetch_20newsgroups(\n",
    "    subset       =  'train', \n",
    "    categories   =  categories,\n",
    "    shuffle      =  True, \n",
    "    random_state =  42,\n",
    "    remove       = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset       =  'test', \n",
    "    categories   =  categories,\n",
    "    shuffle      =  True, \n",
    "    random_state =  42,\n",
    "    remove       =  ('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "\"\"\" Our training data \"\"\"\n",
    "X_train = training_data.data\n",
    "y_train = training_data.target\n",
    "\n",
    "\"\"\" Our testing data\"\"\"\n",
    "X_test = test_data.data\n",
    "y_test = test_data.target\n",
    "\n",
    "# Rememver all that code needed to vectorize our text data before modeling?  \n",
    "# We still need to use it in order to do EDA and evalutate our dataset before we model. \n",
    "# DO NOT GET IN THE HABBIT OF MODELING WITHOUT EDA!\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),     # You will have questions about this\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Fit our data to the pipeline AS IF it were any other model like we've previously done in sklearn\n",
    "# Note:  X_train is literal text data, RAW format!\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reference the vectorized matrix from the \"model\" object cast from the Pipeline instance.\n",
    "Plot the vectorized data with aggregation.  Your previous function will work well if you refactor. \n",
    "\n",
    "_It's helpful to look at different subset classes to understand how each of the features could or may contribute to prediction.  To know which model to use, how it may perform, it's essential to look at your data in order to understand model selection well and confidently evaluate and report findings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are some hints to jumpstart your work\n",
    "\n",
    "# 1. Inspect the pipeline object.  All of the \"steps\" whithin the pipeline are contained inside.\n",
    "pipeline.steps\n",
    "\n",
    "# 2. Use the 2nd step object, \"tfidf\", to get a reference to the object that can transform data\n",
    "# this will get \"TfidfTransformer\" object in the steps list ('tfidf', TfidfTransformer)\n",
    "step2_transformer = pipeline.steps[1][1] \n",
    "\n",
    "# 3. Use the step2_transformer to .fit_transform() and examine your training dataset with proper EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Update the pipeline to use only CountVectorizer.\n",
    "Also experiment different classification models:\n",
    "- KNN\n",
    "- Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
