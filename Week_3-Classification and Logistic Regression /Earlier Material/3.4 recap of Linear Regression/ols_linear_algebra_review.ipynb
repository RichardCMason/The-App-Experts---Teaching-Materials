{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Review: The Normal equation and minimizing error\n",
    "\n",
    "---\n",
    "\n",
    "### The \"Least Squares\" Solution to Linear Regression\n",
    "\n",
    "**Step 1:** With target vector $y$ and prediction matrix $X$, we can formulate a regression as:\n",
    "\n",
    "### $$ y = \\beta X + \\epsilon $$\n",
    "\n",
    "Where $\\beta$ is our vector of coefficients and $\\epsilon$ is our vector of errors, or residuals.\n",
    "\n",
    "**Step 2:** Equivalently, we can formulate this as a calculation of the residuals:\n",
    "\n",
    "### $$ \\epsilon = \\beta X - y $$\n",
    "\n",
    "*Our goal is to minimize the sum of the squared residuals.* This is also known as the \"least squares loss function.\" \n",
    "\n",
    "**Step 3:** Solve for the sum of the squared residuals on the left side of the equation. Recall that the vector of errors are equivalent to the residuals. The sum of the squared residuals is represented as the dot product of the vector of residuals.\n",
    "\n",
    "### $$ \\sum_{i=1}^n \\epsilon_i^2 = \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\cdots \\epsilon_n\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\\\ \\cdots \\\\ \\epsilon_n\n",
    "\\end{array}\\right] = \\epsilon^T \\epsilon\n",
    "$$\n",
    "\n",
    "Therefore we can write the sum of the squared residuals as:\n",
    "\n",
    "### $$ \\epsilon^T \\epsilon = (\\beta X - y)^T (\\beta X - y) $$\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "### $$ \\epsilon^T \\epsilon = y^Ty - y^TX\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta $$\n",
    "\n",
    "**Step 4:** We want to find the coefficients at the loss function^Ts minimum. In this case we can use calculus, taking the derivative with respect to the $\\beta$ vector:\n",
    "\n",
    "### $$ \\frac{\\partial \\epsilon^T \\epsilon}{\\partial \\beta} = \n",
    "-2X^Ty + 2X^TX\\beta$$\n",
    "\n",
    "Because we want to minimize the loss function and the loss function is convex, we set the derivative to zero and solve for the beta coefficient vector:\n",
    "\n",
    "### $$ 0 = -2X^Ty + 2X^TX\\beta \\\\\n",
    "X^TX\\beta = X^Ty \\\\\n",
    "\\beta = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
