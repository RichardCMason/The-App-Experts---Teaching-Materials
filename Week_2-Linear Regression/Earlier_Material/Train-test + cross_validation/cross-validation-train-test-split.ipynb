{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting-underfitting'></a>\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with the first model?**\n",
    "- The underfit model falls short of capturing the complexity of the \"true model\" of the data.\n",
    "\n",
    "**What's wrong with the third model?**\n",
    "- The overfit model is too complex and is modeling random noise in the data.\n",
    "\n",
    "**The middle model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://image.slidesharecdn.com/nncollovcapaldo2013-131220052427-phpapp01/95/machine-learning-introduction-to-neural-networks-12-638.jpg?cb=1393073301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 1:** Why is overfitting a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Check the following in the cells below:\n",
    "1. Do we have any null values?\n",
    "2. Do we have all numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Plot a Heatmap of the Correlation Matrix\n",
    "\n",
    "Heatmaps are an effective way to visually examine the correlational structure of your predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Use seaborn's `.pairplot()` method to create scatterplots for each of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='x-y'></a>\n",
    "\n",
    "### Create our features matrix (`X`) and target vector (`y`)\n",
    "\n",
    "The following columns will be our features:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train/Test Split and Model Validation\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on fitting the best model to our data. But is this the best model for our sample data or the best model overall? How do we know?\n",
    "\n",
    "In practice we need to validate our model's ability to generalize to new data. One popular method for performing model validation is by splitting our data into subsets: data on which we *train* our model and data on which we *test* our model.\n",
    "\n",
    "The most basic type of \"hold-out\" validation is called **train/test split**. We split our data into two pieces:\n",
    "\n",
    "> **\"A Training Set\":** The subset of the data on which we fit our model.\n",
    "\n",
    "> **\"A Testing Set\":** The subset of the data on which we evaluate the quality of our predictions.\n",
    "\n",
    "\n",
    "**Train/Test Split Benefits:**\n",
    "\n",
    "- Testing data can represent \"future\" data; for prediction-oriented models, it's critical to ensure that a model that is performing well on current data will likely perform well on future data.\n",
    "- It can help diagnose and avoid overfitting via model tuning.\n",
    "- It can improve the quality of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "### Scikit-Learn's `train_test_split` Function\n",
    "\n",
    "Performing train/test splits using scikit-learn is easy — load the `train_test_split` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "**Arguments**:\n",
    "- *Arrays*: Any number of arrays/matrices to split up into training and testing sets (they should be the same length).\n",
    "- `test_size`: An integer representing the exact size of the testing subset or a float for a percentage.\n",
    "- `train_size`: Alternatively, you can specify the training size.\n",
    "- `stratify`: Supply a vector to stratify the splitting (by more important classification tasks).\n",
    "- `random_state`: a numerical seed for randomly splitting your data with reproducability\n",
    "\n",
    "**Perform a split of our `X` and `y`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the docs: [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Note that we could always split the data up manually. Here's an example for of manually split code for [this data set](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-on-train'></a>\n",
    "\n",
    "### Linear Regression model\n",
    "\n",
    "In the cell below, create a `LinearRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "## K-Fold Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "K-fold cross-validation takes the idea of a single train/test split and expands it to *multiple tests* across different train/test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80 percent of the data and your testing set will contain the other 20 percent, you could have five different 80/20 splits in which the test set in each is a different set of observations. We have:\n",
    "- Five (K=5) training sets.\n",
    "- Five (K=5) corresponding testing sets.\n",
    "\n",
    "**K-fold cross-validation builds K models — one for each train/test pair — and evaluates those models on each respective test set.**\n",
    "\n",
    "### K-Fold Cross-Validation Visually\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "Cross-validation helps us understand how a model parameterization may perform in a variety of cases. The k-fold cross-validation procedure can be described in pseudocode:\n",
    "\n",
    "```\n",
    "set k\n",
    "create k groups of rows in data\n",
    "\n",
    "for group i in k row groups:\n",
    "    test data is data[group i]\n",
    "    train data is data[all groups not i]\n",
    "    \n",
    "    fit model on train data\n",
    "    \n",
    "    score model on test data\n",
    "    \n",
    "evaluate mean of k model scores\n",
    "evaluate variance of k model scores\n",
    "```\n",
    "\n",
    "Odd case No. 1:\n",
    "> **When K=2**: This is equivalent to performing ***two*** mirror image 50-50 train/test splits.\n",
    "\n",
    "Odd case No. 2:\n",
    "> **When K=number of rows**: This is known as \"leave-one-out cross-validation,\" or LOOCV. A model is built on all but one row and tested on the single excluded observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folds Cross Validation in `sklearn`\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again, scikit-learn provides useful functions to do the heavy lifting. \n",
    "\n",
    "The function `cross_val_score` returns the $R^2$ for each testing set. \n",
    "\n",
    "In the cells below, create a `KFold` object and score your training set (`X_train`, `y_train`) using `cross_val_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold challenge:\n",
    "\n",
    "In the cell bellow, manually recreate the results from `cross_val_score` using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hold-out'></a>\n",
    "\n",
    "## Hold-Out Sets\n",
    "\n",
    "---\n",
    "\n",
    "Hold-out sets are a version of train/test split. To create a hold-out set, you:\n",
    "\n",
    "1) **Split data into a large training and a small testing set. This small testing set will be the \"hold-out\" set.**\n",
    "\n",
    "2) **For a set of different model parameterizations:**\n",
    "\n",
    "    1) Set up the model.\n",
    "    2) Cross-validate the current model on the training data.\n",
    "    3) Save the model performance.\n",
    "\n",
    "3) **Select the model that performed best using cross-validation on the training data.**\n",
    "\n",
    "4) **Perform a final test of that model on the original \"hold-out\" test set.**\n",
    "\n",
    "> **Note:** The \"hold-out\" method is more conservative, but also requires you to have more data. With smaller data sets it can be infeasible.\n",
    "\n",
    "The graphic below explains the hold-out method visually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
