{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## 4.05 - Generalized Linear Models\n",
    "\n",
    "_Authors: Matt Brems (DC) h/t Timothy Book, Justin Pounders (ATL)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models (GLMs)\n",
    "\n",
    "**Generalized linear models** describes a class of models that take the linear model (think linear regression) and generalizes the linear model beyond the assumptions of the linear regression model we discussed last week.\n",
    "\n",
    "In fact, you have already seen an example of a _generalized_ linear model: logistic regression.\n",
    "\n",
    "**Linear regression:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p $$\n",
    "\n",
    "> The predicted variable $\\hat{y}$ will be in the range $(-\\infty, \\infty)$.\n",
    "\n",
    "**Logistic regression:**\n",
    "\n",
    "$$ \\hat{y} = \\sigma\\left(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\sigma(t) = \\frac{e^t}{1 + e^t} $$\n",
    "\n",
    "> The predicted variable $\\hat{y}$ is now in the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **What are examples of things we might predict that don't fall in the range $(-\\infty, \\infty)$ or $(0,1)$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bend the spoon...\n",
    "\n",
    "Generalized linear models are _linear in the coefficients_ but \"bend\" the linear estimator to match the range we want for our target, $Y$.\n",
    "\n",
    "Three ingredients:\n",
    "- The linear piece\n",
    "- The \"bending\" function\n",
    "- A probability distribution for errors\n",
    "\n",
    "Putting these three components together leads to something that looks generically like...\n",
    "\n",
    "$$ Y = g\\left( \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p \\right) + \\varepsilon $$\n",
    "where\n",
    "\n",
    "- $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p$ is the linear piece\n",
    "- $g(\\cdot)$ is the bending piece; called a link function\n",
    "- $\\varepsilon$ is the random \"error\" piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_Let's look at two specific examples that you have already seen!_\n",
    "\n",
    "---\n",
    "\n",
    "**Linear regression:**\n",
    "\n",
    "- Linear piece: $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p$\n",
    "- Link function: $g(t) = t$\n",
    "- Error component: $\\varepsilon \\sim N(0,\\sigma)$\n",
    "\n",
    "![](./images/lin_reg.png)\n",
    "\n",
    "**Logistic regression:**\n",
    "\n",
    "- Linear piece: $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p$\n",
    "- Link function: $g(t) = \\frac{e^t}{1 + e^t}$\n",
    "- Error component: $\\varepsilon \\sim Bernoulli(p)$\n",
    "\n",
    "![](./images/log_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices, choices, choices...\n",
    "\n",
    "Chosing the right kind of generalized linear model (GLM) from all possibilities really boils down to picking the \"error.\"\n",
    "\n",
    "The \"error\" model is really telling you how you expect observations to be distributed.  It is a probability distribution.\n",
    "\n",
    "> 1. In traditional linear regression, the error term is a normal distribution.  This means that you expect actual observations to be normally distributed around your line.\n",
    "\n",
    "> 2. In logistic regression, the error term is a Bernoulli distribution.  This means that you expect actual observations to be above (1) or below (0) the logit curve with a certain probability.\n",
    "\n",
    "Choosing the distribution function often points to a link function you should use: [here is a table](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. If $Y$ is a non-negative integer:\n",
    "   - Poisson regression if mean $\\approx$ variance\n",
    "   - Negative Binomial regression if variance $\\gg$ mean (overdisperse)\n",
    "   - For example,\n",
    "     - Units sold\n",
    "     - Customers through the door\n",
    "     - Patients to the ER\n",
    "     - Number of cars racing the red light\n",
    "2. If $Y$ values represent categories\n",
    "   - Multinomial logistic regression (unordered categories)\n",
    "   - Ordinal logstic regression (ordered categories)\n",
    "   - For example,\n",
    "     - Does a population tend to buy groceries at Whole Foods, Publix or Kroger?\n",
    "     - Will millenials vote democrat, republican or independent?\n",
    "     - Predicting the Amazon star rating of books.\n",
    "3. If $Y$ values are continuous, non-negative\n",
    "   - Gamma regression\n",
    "   - For example,\n",
    "     - How long before my Uber/Lyft gets here?\n",
    "     - Home prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `statsmodels` API\n",
    "\n",
    "We will use the `statsmodels` API to explore GLMs in Python.  (`sklearn` does not have a robust implementation for GLMs.)  Documentation and examples for `statsmodels` can be found [here](http://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels.genmod.generalized_linear_model.GLM).\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "**When do we use it?** When we want to model something on the $\\{0,1\\}$ range... like the probability that a person will develop a disease.\n",
    "\n",
    "> Is logistic regression actually _regression_ or _classification_???\n",
    "\n",
    "### The Data\n",
    "The data used from this example come from one of UCLA's IDRE modules (for R).  The module can be found [here](https://stats.idre.ucla.edu/r/dae/logit-regression/).\n",
    "\n",
    "**Data Description:** _A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don’t admit, is a binary variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n",
    "\n",
    "# Let's do some EDA!\n",
    "print(grad.shape)\n",
    "grad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.admit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grad.boxplot('gre', by='admit')\n",
    "grad.boxplot(['gpa', 'rank'], by='admit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model time...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's build our GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** How would I interpret the coefficient for `rank`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** As the rank of my undergraduate institution increases by 1, my odds of being admitted drop by a factor of 0.57."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** If I scored 100 points better on my GRE, how would that affect my odds of acceptance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** As my GRE score increases by 100 points, my odds of being admitted go up by 26%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- GLMs are typically not \"directly solvable.\"  Instead, a solution is initially guessed then iteratively refined until to you converge on an answer.\n",
    "- The default maximum number of iterations for GLMs is 100. If `No. Iterations` is 100, that means the algorithm probably didn't converge and that the $\\mathbf{\\hat{\\beta}}$ are still changing. Therefore, **your output is unreliable - DO NOT USE IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n",
    "**When do we use it?** When we want to model something on the $\\{0,1,2,\\ldots\\}$ range... like number of objects sold or nunber of awards earned!\n",
    "\n",
    "#### Data\n",
    "We'll again rely on UCLA's IDRE module.  This one can be found [here](https://stats.idre.ucla.edu/r/dae/poisson-regression/).\n",
    "\n",
    "#### Data Description\n",
    "_The number of awards earned by students at one high school. Predictors of the number of awards earned include the type of program in which the student was enrolled (e.g., vocational, general or academic) and the score on their final exam in math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/poisson_sim.csv\")\n",
    "\n",
    "# Let's do some EDA:\n",
    "award.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award.plot('math', 'num_awards', kind = 'scatter')\n",
    "award.boxplot('num_awards', by = 'prog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Notice that prog is actually a categorical variable - I am aware of this.\n",
    "# I'm going to suspsend that knowledge for the sake of example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: For a one-unit increase in `math`, I expect to win $e^{0.0861} \\approx 1.09$ times as many awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma Regression\n",
    "\n",
    "**When do we use it?** When we want to model something on the $[0,\\infty)$ range... like time until some event occurs!\n",
    "\n",
    "### The Data\n",
    "The data used from this example come from a 1945 study about and is inspired by [Peter Craigmile's use](http://www.craigmile.com/peter/teaching/7430/notes/7_gamma_influence.pdf) of this example.\n",
    "\n",
    "**Data Description:** _“Hurn, et al. (1945) published data on the clotting time of blood, giving clotting time in seconds for normal plasma diluted to nine different percentage concentrations with prothrombin-free plasma; clotting was induced by two lots of thromboplastin.” [McCullagh and Nelder](http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clot = pd.read_csv(\"./clotting.csv\", index_col = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clot.boxplot('clot_time', by = 'lot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clot.plot('plasma_pct', 'clot_time', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering, anyone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: For a one-unit increase in `plasma_pct_log10`, I expect the blood will take $e^{-1.4654} \\approx 23\\%$ as much time to clot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(-1.4654)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analysis of Deviance (BONUS)\n",
    "\n",
    "We've spoken briefly about deviance before as a generalization of the sums of squares of error for generalized linear models.\n",
    "\n",
    "Suppose we have two models:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y_{full} &=& \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k + \\cdots + \\beta_pX_p + \\varepsilon \\\\\n",
    "Y_{reduced} &=& \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k + \\varepsilon\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We say that $Y_{reduced}$ is nested in $Y_{full}$, because the reduced model could \"fit inside\" the full. (You can learn more about nested linear regression models [here](http://people.reed.edu/~jones/Courses/P24.pdf), although the ideas approximately hold for generalized linear models as well.)\n",
    "\n",
    "When we have one model nested inside the other, there is a statistical test to see if adding new variables are worth it. (Think about it like looking at the difference in mean squared error or $R^2$ by adding a variable, but getting a $p$-value quantifying whether or not it's worth it!)\n",
    "\n",
    "We calculate the **deviance** of the reduced model and subtract the **deviance** of the full model from it. This difference in deviance follows a [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) with $p-k$ degrees of freedom. (Note that $p-k$ indicates how many variables we took out of our full model to get to the reduced model!)\n",
    "\n",
    "**This comparison only works with nested models! Do not use if your models are not nested!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model differences\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# First, build our top model\n",
    "indep_vars = ['gre', 'gpa', 'rank']\n",
    "X = sm.add_constant(grad[indep_vars])\n",
    "y = grad.admit\n",
    "\n",
    "glm_logit = sm.GLM(y, \n",
    "                   X,\n",
    "                   sm.families.Binomial(sm.families.links.logit))\n",
    "results_logit = glm_logit.fit()\n",
    "\n",
    "\n",
    "# Next, let's see if we can safely reduce our model\n",
    "reduced_vars = ['gre', 'gpa']\n",
    "X_reduced = sm.add_constant(grad[reduced_vars])\n",
    "\n",
    "results_reduced = sm.GLM(y,\n",
    "                 X_reduced,\n",
    "                 sm.families.Binomial(sm.families.links.logit)).fit()\n",
    "results_reduced.summary()\n",
    "\n",
    "\n",
    "# Calculate the difference in deviance\n",
    "D = results_reduced.deviance - results_logit.deviance\n",
    "print('Difference in Deviance: ', D)\n",
    "\n",
    "# Check to see if this difference is significant\n",
    "pval = 1 - chi2.cdf(D, df = 1)\n",
    "print('p-value of test of difference: ', pval) # What can we conclude here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0:$ reduced model is better\n",
    "\n",
    "$H_A:$ reduced model is not better\n",
    "\n",
    "Because $p < \\alpha$, we reject $H_0$ and conclude that the reduced model is not better."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
